{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d55d5319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import tensorflow as tf\n",
    "from openai import OpenAI,AzureOpenAI\n",
    "from summarizer import Summarizer\n",
    "from datasets import load_dataset, load_from_disk, load_metric\n",
    "from transformers import pipeline, set_seed,AutoModelForSeq2SeqLM, AutoTokenizer, BertTokenizer,EncoderDecoderModel\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "504d995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    level=logging.INFO, \n",
    "    filename='./logs/Abstractive Text Summary Generation.log')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57cda761",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"==========================================================================================================\")\n",
    "logging.info(\"Abstractive Text Summarization Start \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8a082b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_OF_TEST_RECORDS = 100\n",
    "logging.info(f\"No of test records - {NO_OF_TEST_RECORDS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0522c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>highlights</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>92c514c913c0bdfe25341af9fd72b29db544099b</td>\n",
       "      <td>Experts question if  packed out planes are put...</td>\n",
       "      <td>Ever noticed how plane seats appear to be gett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2003841c7dc0e7c5b1a248f9cd536d727f27a45a</td>\n",
       "      <td>Drunk teenage boy climbed into lion enclosure ...</td>\n",
       "      <td>A drunk teenage boy had to be rescued by secur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        id  \\\n",
       "0           0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
       "1           1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
       "\n",
       "                                          highlights  \\\n",
       "0  Experts question if  packed out planes are put...   \n",
       "1  Drunk teenage boy climbed into lion enclosure ...   \n",
       "\n",
       "                                             article  \n",
       "0  Ever noticed how plane seats appear to be gett...  \n",
       "1  A drunk teenage boy had to be rescued by secur...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time_ats=time.time()\n",
    "\n",
    "sample_test_df = pd.read_csv('./input/test_cleaned.csv', nrows=NO_OF_TEST_RECORDS)\n",
    "sample_test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df4b758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84a6f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration={}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1693e70d",
   "metadata": {},
   "source": [
    "# Zero Shot Abstractive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e64449c",
   "metadata": {},
   "source": [
    "### Baseline Model Bi LSTM based ENC-DEC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05b24e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roundTS(startTime, endTime):\n",
    "    return round((endTime -startTime),4)\n",
    "\n",
    "def avgTimePerRecord(startTime, endTime, no_of_recs):\n",
    "     return round((endTime -startTime)/no_of_recs ,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1b3556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_abs_baseline(text):\n",
    "    return \"\\n\".join(sent_tokenize(text)[:4]) # First 4 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c3e75c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From D:\\anaconda\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "enc_model = tf.keras.models.load_model('./models/baseline_abs/encoder_modelBiLSTM.h5', compile=True)\n",
    "dec_model = tf.keras.models.load_model('./models/baseline_abs/decoder_modelBiLSTM.h5', compile=True)\n",
    "enc_model.compile()\n",
    "dec_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3644eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_tokenizer = \"\"\n",
    "with open('./models/baseline_abs/s_tokenizerBiLSTM.pkl', 'rb') as f:\n",
    "    s_tokenizer = pickle.load(f)\n",
    "\n",
    "def generate_summary(input_text):\n",
    "\n",
    "    # Tokenize the input text\n",
    "    input_seq = s_tokenizer.texts_to_sequences([input_text])\n",
    "    input_seq = tf.keras.preprocessing.sequence.pad_sequences(input_seq, maxlen=300, padding='post')\n",
    "\n",
    "    # Generate the summary\n",
    "    h, c = enc_model.predict(input_seq,verbose=0)\n",
    "\n",
    "    next_token = np.zeros((1, 1))\n",
    "    next_token[0, 0] = s_tokenizer.word_index['sostok']\n",
    "    output_seq = ''\n",
    "\n",
    "    stop = False\n",
    "    count = 0\n",
    "\n",
    "    while not stop:\n",
    "        if count > 100:\n",
    "            break\n",
    "        decoder_out, state_h, state_c = dec_model.predict([next_token]+[h, c],verbose=0)\n",
    "        token_idx = np.argmax(decoder_out[0, -1, :])\n",
    "\n",
    "        if token_idx == s_tokenizer.word_index['eostok']:\n",
    "            stop = True\n",
    "        elif token_idx > 0 and token_idx != s_tokenizer.word_index['sostok']:\n",
    "            token = s_tokenizer.index_word[token_idx]\n",
    "            output_seq = output_seq + ' ' + token\n",
    "\n",
    "        next_token = np.zeros((1, 1))\n",
    "        next_token[0, 0] = token_idx\n",
    "        h, c = state_h, state_c\n",
    "        count += 1\n",
    "\n",
    "    return output_seq.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b11fed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tokenizer = Tokenizer(num_words=77024)\n",
    "maxlen_text = 300\n",
    "maxlen_summ = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2e160b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = sample_test_df.drop(['id'], axis=1)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e506f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ever noticed how plane seats appear to be gett...</td>\n",
       "      <td>Experts question if  packed out planes are put...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A drunk teenage boy had to be rescued by secur...</td>\n",
       "      <td>Drunk teenage boy climbed into lion enclosure ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               story  \\\n",
       "0  Ever noticed how plane seats appear to be gett...   \n",
       "1  A drunk teenage boy had to be rescued by secur...   \n",
       "\n",
       "                                             summary  \n",
       "0  Experts question if  packed out planes are put...  \n",
       "1  Drunk teenage boy climbed into lion enclosure ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_final_story =[]\n",
    "test_final_summary =[]\n",
    "\n",
    "for i, sty in enumerate(test_data['article']):\n",
    "    sty_len = len(sty.split())\n",
    "    sumy_len = len(test_data['highlights'][i].split())\n",
    "    if (sty_len > sumy_len):\n",
    "        test_final_story.append(sty)\n",
    "        test_final_summary.append(test_data['highlights'][i])\n",
    "print(len(test_final_story), len(test_final_summary))\n",
    "\n",
    "df_testfinal=pd.DataFrame({'story':test_final_story,'summary':test_final_summary})\n",
    "#df_testfinal.to_csv(\"test863.csv\")\n",
    "df_testfinal.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce4702c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = [sent for sent in df_testfinal['story']]\n",
    "test_inputs = t_tokenizer.texts_to_sequences(list(test_inputs))\n",
    "test_inputs = pad_sequences(test_inputs, maxlen=maxlen_text, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3213cc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>summary</th>\n",
       "      <th>baseline-abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ever noticed how plane seats appear to be gett...</td>\n",
       "      <td>Experts question if  packed out planes are put...</td>\n",
       "      <td>police say they are not to be a new police say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A drunk teenage boy had to be rescued by secur...</td>\n",
       "      <td>Drunk teenage boy climbed into lion enclosure ...</td>\n",
       "      <td>police say they are not to be a new york polic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dougie Freedman is on the verge of agreeing a ...</td>\n",
       "      <td>Nottingham Forest are close to extending Dougi...</td>\n",
       "      <td>the new york s office says the new york is the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               story  \\\n",
       "0  Ever noticed how plane seats appear to be gett...   \n",
       "1  A drunk teenage boy had to be rescued by secur...   \n",
       "2  Dougie Freedman is on the verge of agreeing a ...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Experts question if  packed out planes are put...   \n",
       "1  Drunk teenage boy climbed into lion enclosure ...   \n",
       "2  Nottingham Forest are close to extending Dougi...   \n",
       "\n",
       "                                        baseline-abs  \n",
       "0  police say they are not to be a new police say...  \n",
       "1  police say they are not to be a new york polic...  \n",
       "2  the new york s office says the new york is the...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.info(\"Generating Baseline Abstractive Summaries...\")\n",
    "st_baseline_ats=time.time()\n",
    "df_testfinal['baseline-abs'] = df_testfinal['story'].apply(generate_summary)\n",
    "sample_test_df['baseline-abs'] = df_testfinal['baseline-abs'].tolist()\n",
    "end_baseline_ats=time.time()\n",
    "logging.info(f\"Baseline ATS Duration - {roundTS(st_baseline_ats, end_baseline_ats)} seconds\")  \n",
    "df_testfinal.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d70dbfd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlogging.info(\"Generating Baseline Abstractive Summaries...\")\\nst_baseline_ats=time.time()\\nsample_test_df[\\'baseline-abs\\'] = sample_test_df[\\'article\\'].apply(generate_abs_baseline)\\nend_baseline_ats=time.time()\\nlogging.info(f\"Baseline ATS Duration - {roundTS(st_baseline_ats, end_baseline_ats)} seconds\")  \\nsample_test_df.head()\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "logging.info(\"Generating Baseline Abstractive Summaries...\")\n",
    "st_baseline_ats=time.time()\n",
    "sample_test_df['baseline-abs'] = sample_test_df['article'].apply(generate_abs_baseline)\n",
    "end_baseline_ats=time.time()\n",
    "logging.info(f\"Baseline ATS Duration - {roundTS(st_baseline_ats, end_baseline_ats)} seconds\")  \n",
    "sample_test_df.head()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1c511b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration['Baseline'] = avgTimePerRecord(st_baseline_ats, end_baseline_ats, NO_OF_TEST_RECORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "193722a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article: \n",
      " Ever noticed how plane seats appear to be getting smaller and smaller With increasing numbers of people taking to the skies some experts are questioning if having such packed out planes is putting passengers at risk. They say that the shrinking space on aeroplanes is not only uncomfortable it s putting our health and safety in danger. More than squabbling over the arm rest shrinking space on planes putting our health and safety in danger This week a U.S consumer advisory group set up by the Department of Tr\n",
      "baseline-ext: \n",
      " police say they are not to be a new police say they are not to be a police say they are not to be a police say they are not to be a police say they are not to be a man\n"
     ]
    }
   ],
   "source": [
    "print('article: \\n', sample_test_df.iloc[0]['article'][:512])\n",
    "print('baseline-ext: \\n', sample_test_df.iloc[0]['baseline-abs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3ea2db",
   "metadata": {},
   "source": [
    "#### BERT Abstractive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b77a6197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>highlights</th>\n",
       "      <th>article</th>\n",
       "      <th>baseline-abs</th>\n",
       "      <th>bert-base-abs-zs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>92c514c913c0bdfe25341af9fd72b29db544099b</td>\n",
       "      <td>Experts question if  packed out planes are put...</td>\n",
       "      <td>Ever noticed how plane seats appear to be gett...</td>\n",
       "      <td>police say they are not to be a new police say...</td>\n",
       "      <td>ever noticed how plane seats appear to be gett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2003841c7dc0e7c5b1a248f9cd536d727f27a45a</td>\n",
       "      <td>Drunk teenage boy climbed into lion enclosure ...</td>\n",
       "      <td>A drunk teenage boy had to be rescued by secur...</td>\n",
       "      <td>police say they are not to be a new york polic...</td>\n",
       "      <td>a drunk teenage boy had to be rescued by secur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>91b7d2311527f5c2b63a65ca98d21d9c92485149</td>\n",
       "      <td>Nottingham Forest are close to extending Dougi...</td>\n",
       "      <td>Dougie Freedman is on the verge of agreeing a ...</td>\n",
       "      <td>the new york s office says the new york is the...</td>\n",
       "      <td>dougie freedman is on the verge of agreeing a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        id  \\\n",
       "0           0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
       "1           1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
       "2           2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n",
       "\n",
       "                                          highlights  \\\n",
       "0  Experts question if  packed out planes are put...   \n",
       "1  Drunk teenage boy climbed into lion enclosure ...   \n",
       "2  Nottingham Forest are close to extending Dougi...   \n",
       "\n",
       "                                             article  \\\n",
       "0  Ever noticed how plane seats appear to be gett...   \n",
       "1  A drunk teenage boy had to be rescued by secur...   \n",
       "2  Dougie Freedman is on the verge of agreeing a ...   \n",
       "\n",
       "                                        baseline-abs  \\\n",
       "0  police say they are not to be a new police say...   \n",
       "1  police say they are not to be a new york polic...   \n",
       "2  the new york s office says the new york is the...   \n",
       "\n",
       "                                    bert-base-abs-zs  \n",
       "0  ever noticed how plane seats appear to be gett...  \n",
       "1  a drunk teenage boy had to be rescued by secur...  \n",
       "2  dougie freedman is on the verge of agreeing a ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bert-base-uncased model zero shot summarization\n",
    "\n",
    "def generate_abs_bert_zs(text):\n",
    "    pipe = pipeline(\"summarization\", model=\"bert-base-uncased\", max_new_tokens=80)\n",
    "    pipe_out = pipe(text[:512])\n",
    "    #print(pipe_out)\n",
    "    return \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))\n",
    "\n",
    "logging.info(\"Generating BERT Abstractive Summaries (Before Fine Tuning) ...\")    \n",
    "st_bert_zs_ats=time.time()\n",
    "sample_test_df['bert-base-abs-zs'] = sample_test_df['article'].apply(generate_abs_bert_zs)\n",
    "end_bert_zs_ats=time.time()\n",
    "logging.info(f\"BERT ZS ATS Duration - {roundTS(st_bert_zs_ats,end_bert_zs_ats)} seconds\")  \n",
    "sample_test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3dc20d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration['BERT'] = avgTimePerRecord(st_bert_zs_ats, end_bert_zs_ats, NO_OF_TEST_RECORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31500ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article: \n",
      " Ever noticed how plane seats appear to be getting smaller and smaller With increasing numbers of people taking to the skies some experts are questioning if having such packed out planes is putting passengers at risk. They say that the shrinking space on aeroplanes is not only uncomfortable it s putting our health and safety in danger. More than squabbling over the arm rest shrinking space on planes putting our health and safety in danger This week a U.S consumer advisory group set up by the Department of Transportation said at a public hearing that while the government is happy to set standards for animals flying on planes it does not stipulate a minimum amount of space for humans. In a world where animals have more rights to space and food than humans said Charlie Leocha consumer representative on the committee. It is time that the DOT and FAA take a stand for humane treatment of passengers. But could crowding on planes lead to more serious issues than fighting for space in the overhead lockers crashing elbows and seat back kicking Tests conducted by the FAA use planes with a 31 inch pitch a standard which on some airlines has decreased . Many economy seats on United Airlines have 30 inches of room while some airlines offer as little as 28 inches . Cynthia Corbertt a human factors researcher with the Federal Aviation Administration that it conducts tests on how quickly passengers can leave a plane. But these tests are conducted using planes with 31 inches between each row of seats a standard which on some airlines has decreased reported the Detroit News. The distance between two seats from one point on a seat to the same point on the seat behind it is known as the pitch. While most airlines stick to a pitch of 31 inches or above some fall below this. While United Airlines has 30 inches of space Gulf Air economy seats have between 29 and 32 inches Air Asia offers 29 inches and Spirit Airlines offers just 28 inches. British Airways has a seat pitch of 31 inches while easyJet has 29 inches Thomson s short haul seat pitch is 28 inches and Virgin Atlantic s is 30 31.\n",
      "bert-base-abs-zs: \n",
      " ever noticed how plane seats appear to be getting smaller and smaller with increasing numbers of people taking to the skies some experts are questioning if having such packed out planes is putting passengers at risk.\n",
      "they say that the shrinking space on aeroplanes is not only uncomfortable it s putting our health and safety in danger.\n",
      "more than squabbling over the arm rest shrinking space on planes putting our health and safety in danger this week a u. s consumer advisory group set up by the department of tr with the increase... has.............................. some of us are concerned that the more tightly packed and tightly seats are also being used.\n",
      "they are getting smaller.. the space of the plane seats and the seats between the seats and the more tightly packed and tightly\n"
     ]
    }
   ],
   "source": [
    "print('article: \\n', sample_test_df.iloc[0]['article'])\n",
    "print('bert-base-abs-zs: \\n', sample_test_df.iloc[0]['bert-base-abs-zs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a06e41",
   "metadata": {},
   "source": [
    "#### T5 Abstractive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "999cfb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_abs_t5_zs(text):\n",
    "    pipe = pipeline('summarization', model = 't5-small' )\n",
    "    pipe_out = pipe(text)\n",
    "    return 'n'.join(sent_tokenize(pipe_out[0]['summary_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e51dcd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 162. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=81)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (970 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (569 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1120 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1125 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1494 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (807 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2062 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (752 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (968 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1275 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1092 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (991 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1633 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1949 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1285 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (681 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1538 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1430 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (986 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (919 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (560 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (775 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (979 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (783 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (766 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (577 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (784 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (622 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1487 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (582 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (579 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (915 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (810 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (587 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (754 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (643 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (714 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (815 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (902 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (830 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1750 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (835 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2034 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (990 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (890 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (703 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (921 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1026 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1039 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1273 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1191 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (924 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1614 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1899 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1123 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (666 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (750 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (639 > 512). Running this sequence through the model will result in indexing errors\n",
      "Your max_length is set to 200, but your input_length is only 188. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=94)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (938 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1006 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (847 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (630 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1377 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (628 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (616 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (590 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (967 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1694 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (925 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (691 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>highlights</th>\n",
       "      <th>article</th>\n",
       "      <th>baseline-abs</th>\n",
       "      <th>bert-base-abs-zs</th>\n",
       "      <th>t5-small-abs-zs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>92c514c913c0bdfe25341af9fd72b29db544099b</td>\n",
       "      <td>Experts question if  packed out planes are put...</td>\n",
       "      <td>Ever noticed how plane seats appear to be gett...</td>\n",
       "      <td>police say they are not to be a new police say...</td>\n",
       "      <td>ever noticed how plane seats appear to be gett...</td>\n",
       "      <td>some experts are questioning if shrinking spac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2003841c7dc0e7c5b1a248f9cd536d727f27a45a</td>\n",
       "      <td>Drunk teenage boy climbed into lion enclosure ...</td>\n",
       "      <td>A drunk teenage boy had to be rescued by secur...</td>\n",
       "      <td>police say they are not to be a new york polic...</td>\n",
       "      <td>a drunk teenage boy had to be rescued by secur...</td>\n",
       "      <td>Rahul Kumar 17 climbed into the enclosure fenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>91b7d2311527f5c2b63a65ca98d21d9c92485149</td>\n",
       "      <td>Nottingham Forest are close to extending Dougi...</td>\n",
       "      <td>Dougie Freedman is on the verge of agreeing a ...</td>\n",
       "      <td>the new york s office says the new york is the...</td>\n",
       "      <td>dougie freedman is on the verge of agreeing a ...</td>\n",
       "      <td>Dougie Freedman is set to sign a new two year ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        id  \\\n",
       "0           0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
       "1           1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
       "2           2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n",
       "\n",
       "                                          highlights  \\\n",
       "0  Experts question if  packed out planes are put...   \n",
       "1  Drunk teenage boy climbed into lion enclosure ...   \n",
       "2  Nottingham Forest are close to extending Dougi...   \n",
       "\n",
       "                                             article  \\\n",
       "0  Ever noticed how plane seats appear to be gett...   \n",
       "1  A drunk teenage boy had to be rescued by secur...   \n",
       "2  Dougie Freedman is on the verge of agreeing a ...   \n",
       "\n",
       "                                        baseline-abs  \\\n",
       "0  police say they are not to be a new police say...   \n",
       "1  police say they are not to be a new york polic...   \n",
       "2  the new york s office says the new york is the...   \n",
       "\n",
       "                                    bert-base-abs-zs  \\\n",
       "0  ever noticed how plane seats appear to be gett...   \n",
       "1  a drunk teenage boy had to be rescued by secur...   \n",
       "2  dougie freedman is on the verge of agreeing a ...   \n",
       "\n",
       "                                     t5-small-abs-zs  \n",
       "0  some experts are questioning if shrinking spac...  \n",
       "1  Rahul Kumar 17 climbed into the enclosure fenc...  \n",
       "2  Dougie Freedman is set to sign a new two year ...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.info(\"Generating T5 Abstrctive Summaries ...\")    \n",
    "st_t5_zs_ats=time.time()\n",
    "sample_test_df['t5-small-abs-zs'] = sample_test_df['article'].apply(generate_abs_t5_zs)\n",
    "end_t5_zs_ats=time.time()\n",
    "logging.info(f\"T5 ZS ATS Duration - {roundTS(st_t5_zs_ats, end_t5_zs_ats) } seconds\")    \n",
    "sample_test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b60fd0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration['T5'] = avgTimePerRecord(st_t5_zs_ats, end_t5_zs_ats, NO_OF_TEST_RECORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0618a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article: \n",
      " Ever noticed how plane seats appear to be getting smaller and smaller With increasing numbers of people taking to the skies some experts are questioning if having such packed out planes is putting passengers at risk. They say that the shrinking space on aeroplanes is not only uncomfortable it s putting our health and safety in danger. More than squabbling over the arm rest shrinking space on planes putting our health and safety in danger This week a U.S consumer advisory group set up by the Department of Transportation said at a public hearing that while the government is happy to set standards for animals flying on planes it does not stipulate a minimum amount of space for humans. In a world where animals have more rights to space and food than humans said Charlie Leocha consumer representative on the committee. It is time that the DOT and FAA take a stand for humane treatment of passengers. But could crowding on planes lead to more serious issues than fighting for space in the overhead lockers crashing elbows and seat back kicking Tests conducted by the FAA use planes with a 31 inch pitch a standard which on some airlines has decreased . Many economy seats on United Airlines have 30 inches of room while some airlines offer as little as 28 inches . Cynthia Corbertt a human factors researcher with the Federal Aviation Administration that it conducts tests on how quickly passengers can leave a plane. But these tests are conducted using planes with 31 inches between each row of seats a standard which on some airlines has decreased reported the Detroit News. The distance between two seats from one point on a seat to the same point on the seat behind it is known as the pitch. While most airlines stick to a pitch of 31 inches or above some fall below this. While United Airlines has 30 inches of space Gulf Air economy seats have between 29 and 32 inches Air Asia offers 29 inches and Spirit Airlines offers just 28 inches. British Airways has a seat pitch of 31 inches while easyJet has 29 inches Thomson s short haul seat pitch is 28 inches and Virgin Atlantic s is 30 31.\n",
      "t5-small-abs-zs: \n",
      " some experts are questioning if shrinking space on planes is putting our health and safety in danger .na consumer advisory group set up by the department of transportation said it does not stipulate a minimum amount of space for humans .nit is time that the DOT and FAA take a stand for humane treatment .\n"
     ]
    }
   ],
   "source": [
    "print('article: \\n', sample_test_df.iloc[0]['article'])\n",
    "print('t5-small-abs-zs: \\n', sample_test_df.iloc[0]['t5-small-abs-zs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de024a56",
   "metadata": {},
   "source": [
    "#### GPT 3.5 Abstractive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86d74cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    # This is` the default and can be omitted\n",
    "    api_key='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',\n",
    ")\n",
    "AZURE_OPENAI_KEY = \"ee16f3418f0740ce8a1a21e262a839a3\"\n",
    "AZURE_OPENAI_ENDPOINT = \"https://tmap-openai.openai.azure.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb28b010",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "  azure_endpoint = \"https://tmap-openai.openai.azure.com/\", \n",
    "  api_key=\"ee16f3418f0740ce8a1a21e262a839a3\",  \n",
    "  api_version=\"2023-09-15-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68fe922a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_abs_gpt_zs(record, model, temp, max_tokens):\n",
    "    summary = \"\"\n",
    "    \n",
    "    abs_pmt = f\"\"\"\n",
    "                Your task is to create a concise, factual summary, \n",
    "                \n",
    "                by selecting and combining key sentences from  the original text. \n",
    "                \n",
    "                Text is delimited by triple backticks. \n",
    "\n",
    "                TEXT: ```{record['article']}```\n",
    "            \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "        model=model, \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a LLM trained by OpenAI.\"},\n",
    "            {\"role\": \"user\", \"content\": abs_pmt}\n",
    "\n",
    "        ],\n",
    "        max_tokens = max_tokens,\n",
    "        temperature = temp,\n",
    "        n=1,        \n",
    "        )\n",
    "        summary = response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        logging.info(f\"Exception occured - {e}\")\n",
    "        summary = \"ERROR\"\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d28f9aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Generating GPT3.5 Abstractive Summaries (Before Prompt Tuning) ...\") \n",
    "st_gpt35_zs_ats=time.time()\n",
    "sample_test_df['gpt-3.5-abs-zs'] = sample_test_df.apply(lambda rec: generate_abs_gpt_zs(rec, 'gpt35', 0, 60), axis=1)\n",
    "end_gpt35_zs_ats=time.time()\n",
    "logging.info(f\"GPT35 ZS ETS Duration - {roundTS(st_gpt35_zs_ats, end_gpt35_zs_ats)} seconds\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49f4fec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>highlights</th>\n",
       "      <th>article</th>\n",
       "      <th>baseline-abs</th>\n",
       "      <th>bert-base-abs-zs</th>\n",
       "      <th>t5-small-abs-zs</th>\n",
       "      <th>gpt-3.5-abs-zs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>92c514c913c0bdfe25341af9fd72b29db544099b</td>\n",
       "      <td>Experts question if  packed out planes are put...</td>\n",
       "      <td>Ever noticed how plane seats appear to be gett...</td>\n",
       "      <td>police say they are not to be a new police say...</td>\n",
       "      <td>ever noticed how plane seats appear to be gett...</td>\n",
       "      <td>some experts are questioning if shrinking spac...</td>\n",
       "      <td>Experts are concerned that the shrinking space...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2003841c7dc0e7c5b1a248f9cd536d727f27a45a</td>\n",
       "      <td>Drunk teenage boy climbed into lion enclosure ...</td>\n",
       "      <td>A drunk teenage boy had to be rescued by secur...</td>\n",
       "      <td>police say they are not to be a new york polic...</td>\n",
       "      <td>a drunk teenage boy had to be rescued by secur...</td>\n",
       "      <td>Rahul Kumar 17 climbed into the enclosure fenc...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>91b7d2311527f5c2b63a65ca98d21d9c92485149</td>\n",
       "      <td>Nottingham Forest are close to extending Dougi...</td>\n",
       "      <td>Dougie Freedman is on the verge of agreeing a ...</td>\n",
       "      <td>the new york s office says the new york is the...</td>\n",
       "      <td>dougie freedman is on the verge of agreeing a ...</td>\n",
       "      <td>Dougie Freedman is set to sign a new two year ...</td>\n",
       "      <td>Dougie Freedman is close to signing a new two-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        id  \\\n",
       "0           0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
       "1           1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
       "2           2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n",
       "\n",
       "                                          highlights  \\\n",
       "0  Experts question if  packed out planes are put...   \n",
       "1  Drunk teenage boy climbed into lion enclosure ...   \n",
       "2  Nottingham Forest are close to extending Dougi...   \n",
       "\n",
       "                                             article  \\\n",
       "0  Ever noticed how plane seats appear to be gett...   \n",
       "1  A drunk teenage boy had to be rescued by secur...   \n",
       "2  Dougie Freedman is on the verge of agreeing a ...   \n",
       "\n",
       "                                        baseline-abs  \\\n",
       "0  police say they are not to be a new police say...   \n",
       "1  police say they are not to be a new york polic...   \n",
       "2  the new york s office says the new york is the...   \n",
       "\n",
       "                                    bert-base-abs-zs  \\\n",
       "0  ever noticed how plane seats appear to be gett...   \n",
       "1  a drunk teenage boy had to be rescued by secur...   \n",
       "2  dougie freedman is on the verge of agreeing a ...   \n",
       "\n",
       "                                     t5-small-abs-zs  \\\n",
       "0  some experts are questioning if shrinking spac...   \n",
       "1  Rahul Kumar 17 climbed into the enclosure fenc...   \n",
       "2  Dougie Freedman is set to sign a new two year ...   \n",
       "\n",
       "                                      gpt-3.5-abs-zs  \n",
       "0  Experts are concerned that the shrinking space...  \n",
       "1                                               None  \n",
       "2  Dougie Freedman is close to signing a new two-...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dcadd34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration['GPT35'] = avgTimePerRecord(st_gpt35_zs_ats, end_gpt35_zs_ats, NO_OF_TEST_RECORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d53d418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article: \n",
      " Ever noticed how plane seats appear to be getting smaller and smaller With increasing numbers of people taking to the skies some experts are questioning if having such packed out planes is putting pas\n",
      "gpt-3.5-abs-zs: \n",
      " Experts are concerned that the shrinking space on aeroplanes is not only uncomfortable but also putting passengers' health and safety in danger. A US consumer advisory group has said that the government does not stipulate a minimum amount of space for humans on planes, despite setting standards for animals. Tests conducted by the\n"
     ]
    }
   ],
   "source": [
    "print('article: \\n', sample_test_df.iloc[0]['article'][:200])\n",
    "print('gpt-3.5-abs-zs: \\n', sample_test_df.iloc[0]['gpt-3.5-abs-zs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad9f0bc",
   "metadata": {},
   "source": [
    "#### GPT 4 Abstractive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "336ebe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = \"https://tmap-openai.openai.azure.com/\", \n",
    "  api_key=\"ee16f3418f0740ce8a1a21e262a839a3\",  \n",
    "  api_version=\"2023-09-15-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f382260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>highlights</th>\n",
       "      <th>article</th>\n",
       "      <th>baseline-abs</th>\n",
       "      <th>bert-base-abs-zs</th>\n",
       "      <th>t5-small-abs-zs</th>\n",
       "      <th>gpt-3.5-abs-zs</th>\n",
       "      <th>gpt-4-abs-zs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>92c514c913c0bdfe25341af9fd72b29db544099b</td>\n",
       "      <td>Experts question if  packed out planes are put...</td>\n",
       "      <td>Ever noticed how plane seats appear to be gett...</td>\n",
       "      <td>police say they are not to be a new police say...</td>\n",
       "      <td>ever noticed how plane seats appear to be gett...</td>\n",
       "      <td>some experts are questioning if shrinking spac...</td>\n",
       "      <td>Experts are concerned that the shrinking space...</td>\n",
       "      <td>Experts are raising concerns that the decreasi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2003841c7dc0e7c5b1a248f9cd536d727f27a45a</td>\n",
       "      <td>Drunk teenage boy climbed into lion enclosure ...</td>\n",
       "      <td>A drunk teenage boy had to be rescued by secur...</td>\n",
       "      <td>police say they are not to be a new york polic...</td>\n",
       "      <td>a drunk teenage boy had to be rescued by secur...</td>\n",
       "      <td>Rahul Kumar 17 climbed into the enclosure fenc...</td>\n",
       "      <td>None</td>\n",
       "      <td>Rahul Kumar, a 17-year-old intoxicated teenage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>91b7d2311527f5c2b63a65ca98d21d9c92485149</td>\n",
       "      <td>Nottingham Forest are close to extending Dougi...</td>\n",
       "      <td>Dougie Freedman is on the verge of agreeing a ...</td>\n",
       "      <td>the new york s office says the new york is the...</td>\n",
       "      <td>dougie freedman is on the verge of agreeing a ...</td>\n",
       "      <td>Dougie Freedman is set to sign a new two year ...</td>\n",
       "      <td>Dougie Freedman is close to signing a new two-...</td>\n",
       "      <td>Dougie Freedman is close to signing a new two-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        id  \\\n",
       "0           0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
       "1           1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
       "2           2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n",
       "\n",
       "                                          highlights  \\\n",
       "0  Experts question if  packed out planes are put...   \n",
       "1  Drunk teenage boy climbed into lion enclosure ...   \n",
       "2  Nottingham Forest are close to extending Dougi...   \n",
       "\n",
       "                                             article  \\\n",
       "0  Ever noticed how plane seats appear to be gett...   \n",
       "1  A drunk teenage boy had to be rescued by secur...   \n",
       "2  Dougie Freedman is on the verge of agreeing a ...   \n",
       "\n",
       "                                        baseline-abs  \\\n",
       "0  police say they are not to be a new police say...   \n",
       "1  police say they are not to be a new york polic...   \n",
       "2  the new york s office says the new york is the...   \n",
       "\n",
       "                                    bert-base-abs-zs  \\\n",
       "0  ever noticed how plane seats appear to be gett...   \n",
       "1  a drunk teenage boy had to be rescued by secur...   \n",
       "2  dougie freedman is on the verge of agreeing a ...   \n",
       "\n",
       "                                     t5-small-abs-zs  \\\n",
       "0  some experts are questioning if shrinking spac...   \n",
       "1  Rahul Kumar 17 climbed into the enclosure fenc...   \n",
       "2  Dougie Freedman is set to sign a new two year ...   \n",
       "\n",
       "                                      gpt-3.5-abs-zs  \\\n",
       "0  Experts are concerned that the shrinking space...   \n",
       "1                                               None   \n",
       "2  Dougie Freedman is close to signing a new two-...   \n",
       "\n",
       "                                        gpt-4-abs-zs  \n",
       "0  Experts are raising concerns that the decreasi...  \n",
       "1  Rahul Kumar, a 17-year-old intoxicated teenage...  \n",
       "2  Dougie Freedman is close to signing a new two-...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.info(\"Generating GPT4 Abstractive Summaries (Before Prompt Tuning) ...\") \n",
    "st_gpt4_zs_ats=time.time()\n",
    "#sample_test_df['gpt-4-abs-zs'] = sample_test_df['article'].apply(generate_abs_gpt_4_zs)\n",
    "sample_test_df['gpt-4-abs-zs'] = sample_test_df.apply(lambda rec: generate_abs_gpt_zs(rec, 'gpt4', 0, 60), axis=1)\n",
    "end_gpt4_zs_ats=time.time()\n",
    "logging.info(f\"GPT4 ZS ETS Duration - {roundTS(st_gpt4_zs_ats, end_gpt4_zs_ats)} seconds\")    \n",
    "sample_test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56ca92b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration['GPT4'] = avgTimePerRecord(st_gpt4_zs_ats, end_gpt4_zs_ats, NO_OF_TEST_RECORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c62e59ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article: \n",
      " Ever noticed how plane seats appear to be getting smaller and smaller With increasing numbers of people taking to the skies some experts are questioning if having such packed out planes is putting passengers at risk. They say that the shrinking space on aeroplanes is not only uncomfortable it s putting our health and safety in danger. More than squabbling over the arm rest shrinking space on planes putting our health and safety in danger This week a U.S consumer advisory group set up by the Department of Transportation said at a public hearing that while the government is happy to set standards for animals flying on planes it does not stipulate a minimum amount of space for humans. In a world where animals have more rights to space and food than humans said Charlie Leocha consumer representative on the committee. It is time that the DOT and FAA take a stand for humane treatment of passengers. But could crowding on planes lead to more serious issues than fighting for space in the overhead lockers crashing elbows and seat back kicking Tests conducted by the FAA use planes with a 31 inch pitch a standard which on some airlines has decreased . Many economy seats on United Airlines have 30 inches of room while some airlines offer as little as 28 inches . Cynthia Corbertt a human factors researcher with the Federal Aviation Administration that it conducts tests on how quickly passengers can leave a plane. But these tests are conducted using planes with 31 inches between each row of seats a standard which on some airlines has decreased reported the Detroit News. The distance between two seats from one point on a seat to the same point on the seat behind it is known as the pitch. While most airlines stick to a pitch of 31 inches or above some fall below this. While United Airlines has 30 inches of space Gulf Air economy seats have between 29 and 32 inches Air Asia offers 29 inches and Spirit Airlines offers just 28 inches. British Airways has a seat pitch of 31 inches while easyJet has 29 inches Thomson s short haul seat pitch is 28 inches and Virgin Atlantic s is 30 31.\n",
      "gpt-4-abs-zs: \n",
      " Experts are raising concerns that the decreasing space on airplanes is not only uncomfortable but also poses health and safety risks to passengers. A U.S consumer advisory group highlighted that, unlike for animals, the government does not mandate a minimum space for humans on planes. The Federal Aviation Administration (FAA) conducts evacuation\n"
     ]
    }
   ],
   "source": [
    "print('article: \\n', sample_test_df.iloc[0]['article'])\n",
    "print('gpt-4-abs-zs: \\n', sample_test_df.iloc[0]['gpt-4-abs-zs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dcde3cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>highlights</th>\n",
       "      <th>article</th>\n",
       "      <th>baseline-abs</th>\n",
       "      <th>bert-base-abs-zs</th>\n",
       "      <th>t5-small-abs-zs</th>\n",
       "      <th>gpt-3.5-abs-zs</th>\n",
       "      <th>gpt-4-abs-zs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>92c514c913c0bdfe25341af9fd72b29db544099b</td>\n",
       "      <td>Experts question if  packed out planes are put...</td>\n",
       "      <td>Ever noticed how plane seats appear to be gett...</td>\n",
       "      <td>police say they are not to be a new police say...</td>\n",
       "      <td>ever noticed how plane seats appear to be gett...</td>\n",
       "      <td>some experts are questioning if shrinking spac...</td>\n",
       "      <td>Experts are concerned that the shrinking space...</td>\n",
       "      <td>Experts are raising concerns that the decreasi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2003841c7dc0e7c5b1a248f9cd536d727f27a45a</td>\n",
       "      <td>Drunk teenage boy climbed into lion enclosure ...</td>\n",
       "      <td>A drunk teenage boy had to be rescued by secur...</td>\n",
       "      <td>police say they are not to be a new york polic...</td>\n",
       "      <td>a drunk teenage boy had to be rescued by secur...</td>\n",
       "      <td>Rahul Kumar 17 climbed into the enclosure fenc...</td>\n",
       "      <td>None</td>\n",
       "      <td>Rahul Kumar, a 17-year-old intoxicated teenage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>91b7d2311527f5c2b63a65ca98d21d9c92485149</td>\n",
       "      <td>Nottingham Forest are close to extending Dougi...</td>\n",
       "      <td>Dougie Freedman is on the verge of agreeing a ...</td>\n",
       "      <td>the new york s office says the new york is the...</td>\n",
       "      <td>dougie freedman is on the verge of agreeing a ...</td>\n",
       "      <td>Dougie Freedman is set to sign a new two year ...</td>\n",
       "      <td>Dougie Freedman is close to signing a new two-...</td>\n",
       "      <td>Dougie Freedman is close to signing a new two-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        id  \\\n",
       "0           0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
       "1           1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
       "2           2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n",
       "\n",
       "                                          highlights  \\\n",
       "0  Experts question if  packed out planes are put...   \n",
       "1  Drunk teenage boy climbed into lion enclosure ...   \n",
       "2  Nottingham Forest are close to extending Dougi...   \n",
       "\n",
       "                                             article  \\\n",
       "0  Ever noticed how plane seats appear to be gett...   \n",
       "1  A drunk teenage boy had to be rescued by secur...   \n",
       "2  Dougie Freedman is on the verge of agreeing a ...   \n",
       "\n",
       "                                        baseline-abs  \\\n",
       "0  police say they are not to be a new police say...   \n",
       "1  police say they are not to be a new york polic...   \n",
       "2  the new york s office says the new york is the...   \n",
       "\n",
       "                                    bert-base-abs-zs  \\\n",
       "0  ever noticed how plane seats appear to be gett...   \n",
       "1  a drunk teenage boy had to be rescued by secur...   \n",
       "2  dougie freedman is on the verge of agreeing a ...   \n",
       "\n",
       "                                     t5-small-abs-zs  \\\n",
       "0  some experts are questioning if shrinking spac...   \n",
       "1  Rahul Kumar 17 climbed into the enclosure fenc...   \n",
       "2  Dougie Freedman is set to sign a new two year ...   \n",
       "\n",
       "                                      gpt-3.5-abs-zs  \\\n",
       "0  Experts are concerned that the shrinking space...   \n",
       "1                                               None   \n",
       "2  Dougie Freedman is close to signing a new two-...   \n",
       "\n",
       "                                        gpt-4-abs-zs  \n",
       "0  Experts are raising concerns that the decreasi...  \n",
       "1  Rahul Kumar, a 17-year-old intoxicated teenage...  \n",
       "2  Dougie Freedman is close to signing a new two-...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf7a09b",
   "metadata": {},
   "source": [
    "# Fine Tuned - Abstractive Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576072c5",
   "metadata": {},
   "source": [
    "###### Prerequisite - Fine Tuned BERT, T5 Pretrained Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc883fa7",
   "metadata": {},
   "source": [
    "#### BERT Abstractive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3962a865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>highlights</th>\n",
       "      <th>article</th>\n",
       "      <th>baseline-abs</th>\n",
       "      <th>bert-base-abs-zs</th>\n",
       "      <th>t5-small-abs-zs</th>\n",
       "      <th>gpt-3.5-abs-zs</th>\n",
       "      <th>gpt-4-abs-zs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>92c514c913c0bdfe25341af9fd72b29db544099b</td>\n",
       "      <td>Experts question if  packed out planes are put...</td>\n",
       "      <td>Ever noticed how plane seats appear to be gett...</td>\n",
       "      <td>police say they are not to be a new police say...</td>\n",
       "      <td>ever noticed how plane seats appear to be gett...</td>\n",
       "      <td>some experts are questioning if shrinking spac...</td>\n",
       "      <td>Experts are concerned that the shrinking space...</td>\n",
       "      <td>Experts are raising concerns that the decreasi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2003841c7dc0e7c5b1a248f9cd536d727f27a45a</td>\n",
       "      <td>Drunk teenage boy climbed into lion enclosure ...</td>\n",
       "      <td>A drunk teenage boy had to be rescued by secur...</td>\n",
       "      <td>police say they are not to be a new york polic...</td>\n",
       "      <td>a drunk teenage boy had to be rescued by secur...</td>\n",
       "      <td>Rahul Kumar 17 climbed into the enclosure fenc...</td>\n",
       "      <td>None</td>\n",
       "      <td>Rahul Kumar, a 17-year-old intoxicated teenage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>91b7d2311527f5c2b63a65ca98d21d9c92485149</td>\n",
       "      <td>Nottingham Forest are close to extending Dougi...</td>\n",
       "      <td>Dougie Freedman is on the verge of agreeing a ...</td>\n",
       "      <td>the new york s office says the new york is the...</td>\n",
       "      <td>dougie freedman is on the verge of agreeing a ...</td>\n",
       "      <td>Dougie Freedman is set to sign a new two year ...</td>\n",
       "      <td>Dougie Freedman is close to signing a new two-...</td>\n",
       "      <td>Dougie Freedman is close to signing a new two-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>caabf9cbdf96eb1410295a673e953d304391bfbb</td>\n",
       "      <td>Fiorentina goalkeeper Neto has been linked wit...</td>\n",
       "      <td>Liverpool target Neto is also wanted by PSG an...</td>\n",
       "      <td>the man is a new york city of the city of the ...</td>\n",
       "      <td>liverpool target neto is also wanted by psg an...</td>\n",
       "      <td>neto is wanted by a number of top european clu...</td>\n",
       "      <td>Liverpool faces competition from PSG and Spani...</td>\n",
       "      <td>Liverpool is interested in signing Fiorentina ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3da746a7d9afcaa659088c8366ef6347fe6b53ea</td>\n",
       "      <td>Tell-all interview with the reality TV star, 6...</td>\n",
       "      <td>Bruce Jenner will break his silence in a two h...</td>\n",
       "      <td>the woman s mother says the girl s mother says...</td>\n",
       "      <td>bruce jenner will break his silence in a two h...</td>\n",
       "      <td>the former Olympian will speak in a two hour i...</td>\n",
       "      <td>Bruce Jenner, the former Olympian and reality ...</td>\n",
       "      <td>Bruce Jenner will discuss his life in a two-ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>64ee7c9eb9f1efbb7da0ce80498434c623615b84</td>\n",
       "      <td>Zlatan Ibrahimovic will line up against former...</td>\n",
       "      <td>As Zlatan Ibrahimovic famously believes the Wo...</td>\n",
       "      <td>the man says the woman is a woman to be a new ...</td>\n",
       "      <td>as zlatan ibrahimovic famously believes the wo...</td>\n",
       "      <td>Zlatan Ibrahimovic will take centre stage agai...</td>\n",
       "      <td>Zlatan Ibrahimovic, the PSG striker, is set to...</td>\n",
       "      <td>Zlatan Ibrahimovic, known for his confidence a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>5cf4682cd03238d5867027ce9492b626cd1ed011</td>\n",
       "      <td>Jameela Jamil, 29, is convinced dental work tr...</td>\n",
       "      <td>Jameela spent GBP3 000 on having all her amalg...</td>\n",
       "      <td>police say they are not to be a new york polic...</td>\n",
       "      <td>jameela spent gbp3 000 on having all her amalg...</td>\n",
       "      <td>dental amalgam has been used for more than 150...</td>\n",
       "      <td>Jameela Jamil, a television presenter and form...</td>\n",
       "      <td>Jameela Jamil, a 29-year-old television presen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>3815d19af18ff22be6ad6095722d7367bb7271af</td>\n",
       "      <td>Christopher Bridger, 25, attacked three women ...</td>\n",
       "      <td>A paramedic who pretended he was gay to get cl...</td>\n",
       "      <td>the man s body was found in the hospital in th...</td>\n",
       "      <td>a paramedic who pretended he was gay to get cl...</td>\n",
       "      <td>Christopher Bridger 25, from Stevenage Hertfor...</td>\n",
       "      <td>Christopher Bridger, a paramedic from Stevenag...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>fb207604ffa7e8371c622840445825db8993d4d2</td>\n",
       "      <td>Paris Saint-Germain captain Thiago Silva suffe...</td>\n",
       "      <td>Paris Saint Germain face Nice on Saturday hopi...</td>\n",
       "      <td>police say they are not to be a new york city ...</td>\n",
       "      <td>paris saint germain face nice on saturday hopi...</td>\n",
       "      <td>Thiago Silva is recovering at home from a thig...</td>\n",
       "      <td>Paris Saint Germain will face Nice on Saturday...</td>\n",
       "      <td>Paris Saint Germain, missing key players inclu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>d25d52c434a13c1df5faa593e8a097d2f501a2b6</td>\n",
       "      <td>.50-caliber bullets equipped with optical sens...</td>\n",
       "      <td>CNN You know the phrase dodging a bullet Forge...</td>\n",
       "      <td>the man was arrested in the scene of the incid...</td>\n",
       "      <td>cnn you know the phrase dodging a bullet forge...</td>\n",
       "      <td>the smart bullets .50 caliber projectiles pass...</td>\n",
       "      <td>The US military has made significant progress ...</td>\n",
       "      <td>The U.S. military has made significant advance...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                        id  \\\n",
       "0            0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
       "1            1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
       "2            2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n",
       "3            3  caabf9cbdf96eb1410295a673e953d304391bfbb   \n",
       "4            4  3da746a7d9afcaa659088c8366ef6347fe6b53ea   \n",
       "..         ...                                       ...   \n",
       "95          95  64ee7c9eb9f1efbb7da0ce80498434c623615b84   \n",
       "96          96  5cf4682cd03238d5867027ce9492b626cd1ed011   \n",
       "97          97  3815d19af18ff22be6ad6095722d7367bb7271af   \n",
       "98          98  fb207604ffa7e8371c622840445825db8993d4d2   \n",
       "99          99  d25d52c434a13c1df5faa593e8a097d2f501a2b6   \n",
       "\n",
       "                                           highlights  \\\n",
       "0   Experts question if  packed out planes are put...   \n",
       "1   Drunk teenage boy climbed into lion enclosure ...   \n",
       "2   Nottingham Forest are close to extending Dougi...   \n",
       "3   Fiorentina goalkeeper Neto has been linked wit...   \n",
       "4   Tell-all interview with the reality TV star, 6...   \n",
       "..                                                ...   \n",
       "95  Zlatan Ibrahimovic will line up against former...   \n",
       "96  Jameela Jamil, 29, is convinced dental work tr...   \n",
       "97  Christopher Bridger, 25, attacked three women ...   \n",
       "98  Paris Saint-Germain captain Thiago Silva suffe...   \n",
       "99  .50-caliber bullets equipped with optical sens...   \n",
       "\n",
       "                                              article  \\\n",
       "0   Ever noticed how plane seats appear to be gett...   \n",
       "1   A drunk teenage boy had to be rescued by secur...   \n",
       "2   Dougie Freedman is on the verge of agreeing a ...   \n",
       "3   Liverpool target Neto is also wanted by PSG an...   \n",
       "4   Bruce Jenner will break his silence in a two h...   \n",
       "..                                                ...   \n",
       "95  As Zlatan Ibrahimovic famously believes the Wo...   \n",
       "96  Jameela spent GBP3 000 on having all her amalg...   \n",
       "97  A paramedic who pretended he was gay to get cl...   \n",
       "98  Paris Saint Germain face Nice on Saturday hopi...   \n",
       "99  CNN You know the phrase dodging a bullet Forge...   \n",
       "\n",
       "                                         baseline-abs  \\\n",
       "0   police say they are not to be a new police say...   \n",
       "1   police say they are not to be a new york polic...   \n",
       "2   the new york s office says the new york is the...   \n",
       "3   the man is a new york city of the city of the ...   \n",
       "4   the woman s mother says the girl s mother says...   \n",
       "..                                                ...   \n",
       "95  the man says the woman is a woman to be a new ...   \n",
       "96  police say they are not to be a new york polic...   \n",
       "97  the man s body was found in the hospital in th...   \n",
       "98  police say they are not to be a new york city ...   \n",
       "99  the man was arrested in the scene of the incid...   \n",
       "\n",
       "                                     bert-base-abs-zs  \\\n",
       "0   ever noticed how plane seats appear to be gett...   \n",
       "1   a drunk teenage boy had to be rescued by secur...   \n",
       "2   dougie freedman is on the verge of agreeing a ...   \n",
       "3   liverpool target neto is also wanted by psg an...   \n",
       "4   bruce jenner will break his silence in a two h...   \n",
       "..                                                ...   \n",
       "95  as zlatan ibrahimovic famously believes the wo...   \n",
       "96  jameela spent gbp3 000 on having all her amalg...   \n",
       "97  a paramedic who pretended he was gay to get cl...   \n",
       "98  paris saint germain face nice on saturday hopi...   \n",
       "99  cnn you know the phrase dodging a bullet forge...   \n",
       "\n",
       "                                      t5-small-abs-zs  \\\n",
       "0   some experts are questioning if shrinking spac...   \n",
       "1   Rahul Kumar 17 climbed into the enclosure fenc...   \n",
       "2   Dougie Freedman is set to sign a new two year ...   \n",
       "3   neto is wanted by a number of top european clu...   \n",
       "4   the former Olympian will speak in a two hour i...   \n",
       "..                                                ...   \n",
       "95  Zlatan Ibrahimovic will take centre stage agai...   \n",
       "96  dental amalgam has been used for more than 150...   \n",
       "97  Christopher Bridger 25, from Stevenage Hertfor...   \n",
       "98  Thiago Silva is recovering at home from a thig...   \n",
       "99  the smart bullets .50 caliber projectiles pass...   \n",
       "\n",
       "                                       gpt-3.5-abs-zs  \\\n",
       "0   Experts are concerned that the shrinking space...   \n",
       "1                                                None   \n",
       "2   Dougie Freedman is close to signing a new two-...   \n",
       "3   Liverpool faces competition from PSG and Spani...   \n",
       "4   Bruce Jenner, the former Olympian and reality ...   \n",
       "..                                                ...   \n",
       "95  Zlatan Ibrahimovic, the PSG striker, is set to...   \n",
       "96  Jameela Jamil, a television presenter and form...   \n",
       "97  Christopher Bridger, a paramedic from Stevenag...   \n",
       "98  Paris Saint Germain will face Nice on Saturday...   \n",
       "99  The US military has made significant progress ...   \n",
       "\n",
       "                                         gpt-4-abs-zs  \n",
       "0   Experts are raising concerns that the decreasi...  \n",
       "1   Rahul Kumar, a 17-year-old intoxicated teenage...  \n",
       "2   Dougie Freedman is close to signing a new two-...  \n",
       "3   Liverpool is interested in signing Fiorentina ...  \n",
       "4   Bruce Jenner will discuss his life in a two-ho...  \n",
       "..                                                ...  \n",
       "95  Zlatan Ibrahimovic, known for his confidence a...  \n",
       "96  Jameela Jamil, a 29-year-old television presen...  \n",
       "97                                               None  \n",
       "98  Paris Saint Germain, missing key players inclu...  \n",
       "99  The U.S. military has made significant advance...  \n",
       "\n",
       "[100 rows x 9 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "596d46c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c6c3a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>highlights</th>\n",
       "      <th>article</th>\n",
       "      <th>baseline-abs</th>\n",
       "      <th>bert-base-abs-zs</th>\n",
       "      <th>t5-small-abs-zs</th>\n",
       "      <th>gpt-3.5-abs-zs</th>\n",
       "      <th>gpt-4-abs-zs</th>\n",
       "      <th>bert-base-abs-ft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>92c514c913c0bdfe25341af9fd72b29db544099b</td>\n",
       "      <td>Experts question if  packed out planes are put...</td>\n",
       "      <td>Ever noticed how plane seats appear to be gett...</td>\n",
       "      <td>police say they are not to be a new police say...</td>\n",
       "      <td>ever noticed how plane seats appear to be gett...</td>\n",
       "      <td>some experts are questioning if shrinking spac...</td>\n",
       "      <td>Experts are concerned that the shrinking space...</td>\n",
       "      <td>Experts are raising concerns that the decreasi...</td>\n",
       "      <td>the dot and faa are happy to set standards for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2003841c7dc0e7c5b1a248f9cd536d727f27a45a</td>\n",
       "      <td>Drunk teenage boy climbed into lion enclosure ...</td>\n",
       "      <td>A drunk teenage boy had to be rescued by secur...</td>\n",
       "      <td>police say they are not to be a new york polic...</td>\n",
       "      <td>a drunk teenage boy had to be rescued by secur...</td>\n",
       "      <td>Rahul Kumar 17 climbed into the enclosure fenc...</td>\n",
       "      <td>None</td>\n",
       "      <td>Rahul Kumar, a 17-year-old intoxicated teenage...</td>\n",
       "      <td>rahul kumar 17 climbed into the enclosure fenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>91b7d2311527f5c2b63a65ca98d21d9c92485149</td>\n",
       "      <td>Nottingham Forest are close to extending Dougi...</td>\n",
       "      <td>Dougie Freedman is on the verge of agreeing a ...</td>\n",
       "      <td>the new york s office says the new york is the...</td>\n",
       "      <td>dougie freedman is on the verge of agreeing a ...</td>\n",
       "      <td>Dougie Freedman is set to sign a new two year ...</td>\n",
       "      <td>Dougie Freedman is close to signing a new two-...</td>\n",
       "      <td>Dougie Freedman is close to signing a new two-...</td>\n",
       "      <td>dougie freedman is set to sign a new two year ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        id  \\\n",
       "0           0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
       "1           1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
       "2           2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n",
       "\n",
       "                                          highlights  \\\n",
       "0  Experts question if  packed out planes are put...   \n",
       "1  Drunk teenage boy climbed into lion enclosure ...   \n",
       "2  Nottingham Forest are close to extending Dougi...   \n",
       "\n",
       "                                             article  \\\n",
       "0  Ever noticed how plane seats appear to be gett...   \n",
       "1  A drunk teenage boy had to be rescued by secur...   \n",
       "2  Dougie Freedman is on the verge of agreeing a ...   \n",
       "\n",
       "                                        baseline-abs  \\\n",
       "0  police say they are not to be a new police say...   \n",
       "1  police say they are not to be a new york polic...   \n",
       "2  the new york s office says the new york is the...   \n",
       "\n",
       "                                    bert-base-abs-zs  \\\n",
       "0  ever noticed how plane seats appear to be gett...   \n",
       "1  a drunk teenage boy had to be rescued by secur...   \n",
       "2  dougie freedman is on the verge of agreeing a ...   \n",
       "\n",
       "                                     t5-small-abs-zs  \\\n",
       "0  some experts are questioning if shrinking spac...   \n",
       "1  Rahul Kumar 17 climbed into the enclosure fenc...   \n",
       "2  Dougie Freedman is set to sign a new two year ...   \n",
       "\n",
       "                                      gpt-3.5-abs-zs  \\\n",
       "0  Experts are concerned that the shrinking space...   \n",
       "1                                               None   \n",
       "2  Dougie Freedman is close to signing a new two-...   \n",
       "\n",
       "                                        gpt-4-abs-zs  \\\n",
       "0  Experts are raising concerns that the decreasi...   \n",
       "1  Rahul Kumar, a 17-year-old intoxicated teenage...   \n",
       "2  Dougie Freedman is close to signing a new two-...   \n",
       "\n",
       "                                    bert-base-abs-ft  \n",
       "0  the dot and faa are happy to set standards for...  \n",
       "1  rahul kumar 17 climbed into the enclosure fenc...  \n",
       "2  dougie freedman is set to sign a new two year ...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(\"./models/bert-base-cnn-finetuned\")\n",
    "bert_model_ft = EncoderDecoderModel.from_pretrained(\"./models/bert-base-cnn-finetuned\")\n",
    "bert_model_ft.to(device)\n",
    "\n",
    "\n",
    "def generate_abs_bert_ft(text):\n",
    "    # cut off at BERT max length 512\n",
    "    inputs = bert_tokenizer([text], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "    output = bert_model_ft.generate(input_ids, attention_mask=attention_mask)\n",
    "    return bert_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "logging.info(\"Generating BERT Abstractive Summaries (After Fine Tuning) ...\")\n",
    "st_bert_ft_ats=time.time()                   \n",
    "sample_test_df['bert-base-abs-ft'] = sample_test_df['article'].apply(generate_abs_bert_ft)\n",
    "end_bert_ft_ats=time.time()\n",
    "logging.info(f\"BERT FT ATS Duration - {roundTS(st_bert_ft_ats, end_bert_ft_ats)} seconds\")  \n",
    "sample_test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1e5ac0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration['BERT-FT'] = avgTimePerRecord(st_bert_ft_ats, end_bert_ft_ats,NO_OF_TEST_RECORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38001e0",
   "metadata": {},
   "source": [
    "#### T5 Abstractive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f46765e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invoking HuggingFace custom finetuned T5 model from local folder\n",
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"./models/t5-small-cnn-hf-finetuned\")\n",
    "text = \"summarize: The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes.\"\n",
    "\n",
    "def generate_abs_t5_ft(text):\n",
    "    response = summarizer(text)\n",
    "    return response[0]['summary_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc7cdafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"T5 Model Fine Tuned Arch: \\n{summarizer.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d5a12bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 162. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=81)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (970 > 512). Running this sequence through the model will result in indexing errors\n",
      "Your max_length is set to 200, but your input_length is only 188. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=94)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>highlights</th>\n",
       "      <th>article</th>\n",
       "      <th>baseline-abs</th>\n",
       "      <th>bert-base-abs-zs</th>\n",
       "      <th>t5-small-abs-zs</th>\n",
       "      <th>gpt-3.5-abs-zs</th>\n",
       "      <th>gpt-4-abs-zs</th>\n",
       "      <th>bert-base-abs-ft</th>\n",
       "      <th>t5-small-abs-ft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>92c514c913c0bdfe25341af9fd72b29db544099b</td>\n",
       "      <td>Experts question if  packed out planes are put...</td>\n",
       "      <td>Ever noticed how plane seats appear to be gett...</td>\n",
       "      <td>police say they are not to be a new police say...</td>\n",
       "      <td>ever noticed how plane seats appear to be gett...</td>\n",
       "      <td>some experts are questioning if shrinking spac...</td>\n",
       "      <td>Experts are concerned that the shrinking space...</td>\n",
       "      <td>Experts are raising concerns that the decreasi...</td>\n",
       "      <td>the dot and faa are happy to set standards for...</td>\n",
       "      <td>A U.S consumer advisory group set up by the De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2003841c7dc0e7c5b1a248f9cd536d727f27a45a</td>\n",
       "      <td>Drunk teenage boy climbed into lion enclosure ...</td>\n",
       "      <td>A drunk teenage boy had to be rescued by secur...</td>\n",
       "      <td>police say they are not to be a new york polic...</td>\n",
       "      <td>a drunk teenage boy had to be rescued by secur...</td>\n",
       "      <td>Rahul Kumar 17 climbed into the enclosure fenc...</td>\n",
       "      <td>None</td>\n",
       "      <td>Rahul Kumar, a 17-year-old intoxicated teenage...</td>\n",
       "      <td>rahul kumar 17 climbed into the enclosure fenc...</td>\n",
       "      <td>Rahul Kumar 17 climbed into a lions enclosure ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>91b7d2311527f5c2b63a65ca98d21d9c92485149</td>\n",
       "      <td>Nottingham Forest are close to extending Dougi...</td>\n",
       "      <td>Dougie Freedman is on the verge of agreeing a ...</td>\n",
       "      <td>the new york s office says the new york is the...</td>\n",
       "      <td>dougie freedman is on the verge of agreeing a ...</td>\n",
       "      <td>Dougie Freedman is set to sign a new two year ...</td>\n",
       "      <td>Dougie Freedman is close to signing a new two-...</td>\n",
       "      <td>Dougie Freedman is close to signing a new two-...</td>\n",
       "      <td>dougie freedman is set to sign a new two year ...</td>\n",
       "      <td>Dougie Freedman is set to sign a new two year ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        id  \\\n",
       "0           0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
       "1           1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
       "2           2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n",
       "\n",
       "                                          highlights  \\\n",
       "0  Experts question if  packed out planes are put...   \n",
       "1  Drunk teenage boy climbed into lion enclosure ...   \n",
       "2  Nottingham Forest are close to extending Dougi...   \n",
       "\n",
       "                                             article  \\\n",
       "0  Ever noticed how plane seats appear to be gett...   \n",
       "1  A drunk teenage boy had to be rescued by secur...   \n",
       "2  Dougie Freedman is on the verge of agreeing a ...   \n",
       "\n",
       "                                        baseline-abs  \\\n",
       "0  police say they are not to be a new police say...   \n",
       "1  police say they are not to be a new york polic...   \n",
       "2  the new york s office says the new york is the...   \n",
       "\n",
       "                                    bert-base-abs-zs  \\\n",
       "0  ever noticed how plane seats appear to be gett...   \n",
       "1  a drunk teenage boy had to be rescued by secur...   \n",
       "2  dougie freedman is on the verge of agreeing a ...   \n",
       "\n",
       "                                     t5-small-abs-zs  \\\n",
       "0  some experts are questioning if shrinking spac...   \n",
       "1  Rahul Kumar 17 climbed into the enclosure fenc...   \n",
       "2  Dougie Freedman is set to sign a new two year ...   \n",
       "\n",
       "                                      gpt-3.5-abs-zs  \\\n",
       "0  Experts are concerned that the shrinking space...   \n",
       "1                                               None   \n",
       "2  Dougie Freedman is close to signing a new two-...   \n",
       "\n",
       "                                        gpt-4-abs-zs  \\\n",
       "0  Experts are raising concerns that the decreasi...   \n",
       "1  Rahul Kumar, a 17-year-old intoxicated teenage...   \n",
       "2  Dougie Freedman is close to signing a new two-...   \n",
       "\n",
       "                                    bert-base-abs-ft  \\\n",
       "0  the dot and faa are happy to set standards for...   \n",
       "1  rahul kumar 17 climbed into the enclosure fenc...   \n",
       "2  dougie freedman is set to sign a new two year ...   \n",
       "\n",
       "                                     t5-small-abs-ft  \n",
       "0  A U.S consumer advisory group set up by the De...  \n",
       "1  Rahul Kumar 17 climbed into a lions enclosure ...  \n",
       "2  Dougie Freedman is set to sign a new two year ...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.info(\"Generating T5 Abstract Summaries (After Prompt Tuning) ...\") \n",
    "st_t5_ft_ats=time.time()\n",
    "sample_test_df['t5-small-abs-ft'] = sample_test_df['article'].apply(generate_abs_t5_ft)\n",
    "end_t5_ft_ats=time.time()\n",
    "logging.info(f\"GPT35 FT ETS Duration - {roundTS(st_t5_ft_ats, end_t5_ft_ats)} seconds\")\n",
    "sample_test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "64e259b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration['T5-FT'] = avgTimePerRecord(st_t5_ft_ats, end_t5_ft_ats,NO_OF_TEST_RECORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "639dcb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A U.S consumer advisory group set up by the Department of Transportation said that while the government is happy to set standards for animals flying on planes it does not stipulate a minimum amount of space for humans . It is time that the DOT and FAA take a stand for humane treatment of passengers . But these tests are conducted using planes with a 31 inch pitch a standard which on some airlines has decreased .'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_df.iloc[0]['t5-small-abs-ft']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0071e6cc",
   "metadata": {},
   "source": [
    "#### GPT 3.5 Abstractive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "38c3dc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computation of Length of article and hughlights\n",
    "def article_len(row):\n",
    "    return len(row['article'].split())\n",
    "\n",
    "def highlights_len(row):\n",
    "    return len(row['highlights'].split())\n",
    "\n",
    "\n",
    "sample_test_df['article_len'] = sample_test_df.apply(lambda r: article_len(r), axis= 1)\n",
    "sample_test_df['highlights_len'] = sample_test_df.apply(lambda r: highlights_len(r), axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e3eef9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_article_size_df = sample_test_df.sort_values('article_len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6121bae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_GPT_Abs_1_Shot(record, model, temp, max_tokens):\n",
    "    summary = \"\"\n",
    "    abs_pmt = f\"\"\"\n",
    "                Your task is to create a concise, factual summary, \n",
    "                \n",
    "                by selecting and combining key sentences from  the original text. \n",
    "                \n",
    "                Text is delimited by triple backticks. \n",
    "\n",
    "                TEXT: ```{record['article']}```\n",
    "            \"\"\"\n",
    "    \n",
    "    #create a concise summary by selecting and combining key sentences from the original text : {text}\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "        model=model, # model = \"deployment_name\".\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a LLM trained by OpenAI.\"},\n",
    "            {\"role\":\"user\",\"content\":sorted_by_article_size_df.iloc[0]['article']},\n",
    "            {\"role\":\"assistant\",\"content\":sorted_by_article_size_df.iloc[0]['highlights']},\n",
    "            {\"role\": \"user\", \"content\": abs_pmt},\n",
    "            ],\n",
    "        max_tokens = max_tokens,\n",
    "        temperature = temp, #top_p=.9\n",
    "        n=1,\n",
    "        )\n",
    "        summary = response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        logging.info(\"Exception occured - {e}\") \n",
    "        summary = \"ERROR\"\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def generate_GPT_Abs_3_Shots(record, model, temp, max_tokens):\n",
    "    summary = \"\"\n",
    "    \n",
    "    abs_pmt = f\"\"\"\n",
    "                Your task is to create a concise, factual summary, \n",
    "                \n",
    "                by selecting and combining key sentences from  the original text. \n",
    "                \n",
    "                Text is delimited by triple backticks. \n",
    "\n",
    "                TEXT: ```{record['article']}```\n",
    "            \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "        model=model, \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a LLM trained by OpenAI.\"},\n",
    "\n",
    "            {\"role\":\"user\",\"content\":sorted_by_article_size_df.iloc[0]['article']},\n",
    "            {\"role\":\"assistant\",\"content\":sorted_by_article_size_df.iloc[0]['highlights']},\n",
    "            \n",
    "            {\"role\":\"user\",\"content\":sorted_by_article_size_df.iloc[1]['article']},\n",
    "            {\"role\":\"assistant\",\"content\":sorted_by_article_size_df.iloc[1]['highlights']},\n",
    "\n",
    "            {\"role\":\"user\",\"content\":sorted_by_article_size_df.iloc[2]['article']},\n",
    "            {\"role\":\"assistant\",\"content\":sorted_by_article_size_df.iloc[2]['highlights']},\n",
    "            \n",
    "            {\"role\": \"user\", \"content\": abs_pmt},\n",
    "            \n",
    "        ],\n",
    "        max_tokens = max_tokens,\n",
    "        temperature = temp,\n",
    "        n=1,\n",
    "        )\n",
    "        summary = response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        logging.info(\"Exception occured - {e}\") \n",
    "        summary = \"ERROR\"\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def generate_GPT_Abs_5_Shots(record, model, temp, max_tokens):\n",
    "    summary = \"\"\n",
    "       \n",
    "    abs_pmt = f\"\"\"\n",
    "                Your task is to create a concise, factual summary, \n",
    "\n",
    "                by selecting and combining key sentences from  the original text. \n",
    "\n",
    "                Text is delimited by triple backticks. \n",
    "\n",
    "                TEXT: ```{record['article']}```\n",
    "            \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model= model, # model = \"deployment_name\".\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a LLM trained by OpenAI.\"},\n",
    "\n",
    "                {\"role\":\"user\",\"content\":sorted_by_article_size_df.iloc[0]['article']},\n",
    "                {\"role\":\"assistant\",\"content\":sorted_by_article_size_df.iloc[0]['highlights']},\n",
    "\n",
    "                {\"role\":\"user\",\"content\":sorted_by_article_size_df.iloc[1]['article']},\n",
    "                {\"role\":\"assistant\",\"content\":sorted_by_article_size_df.iloc[1]['highlights']},\n",
    "\n",
    "                {\"role\":\"user\",\"content\":sorted_by_article_size_df.iloc[2]['article']},\n",
    "                {\"role\":\"assistant\",\"content\":sorted_by_article_size_df.iloc[2]['highlights']},\n",
    "\n",
    "                {\"role\":\"user\",\"content\":sorted_by_article_size_df.iloc[3]['article']},\n",
    "                {\"role\":\"assistant\",\"content\":sorted_by_article_size_df.iloc[3]['highlights']},\n",
    "\n",
    "                {\"role\":\"user\",\"content\":sorted_by_article_size_df.iloc[4]['article']},\n",
    "                {\"role\":\"assistant\",\"content\":sorted_by_article_size_df.iloc[4]['highlights']},\n",
    "\n",
    "                {\"role\": \"user\", \"content\": abs_pmt},\n",
    "\n",
    "            ],\n",
    "            max_tokens = max_tokens,\n",
    "            temperature = temp, #top_p=.9\n",
    "            n=1,\n",
    "            )\n",
    "        summary = response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        logging.info(\"Exception occured - {e}\") \n",
    "        summary = \"ERROR\"\n",
    "  \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e6147758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>highlights</th>\n",
       "      <th>article</th>\n",
       "      <th>baseline-abs</th>\n",
       "      <th>bert-base-abs-zs</th>\n",
       "      <th>t5-small-abs-zs</th>\n",
       "      <th>gpt-3.5-abs-zs</th>\n",
       "      <th>gpt-4-abs-zs</th>\n",
       "      <th>bert-base-abs-ft</th>\n",
       "      <th>t5-small-abs-ft</th>\n",
       "      <th>article_len</th>\n",
       "      <th>highlights_len</th>\n",
       "      <th>gpt-3.5-abs-ft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>3815d19af18ff22be6ad6095722d7367bb7271af</td>\n",
       "      <td>Christopher Bridger, 25, attacked three women ...</td>\n",
       "      <td>A paramedic who pretended he was gay to get cl...</td>\n",
       "      <td>the man s body was found in the hospital in th...</td>\n",
       "      <td>a paramedic who pretended he was gay to get cl...</td>\n",
       "      <td>Christopher Bridger 25, from Stevenage Hertfor...</td>\n",
       "      <td>Christopher Bridger, a paramedic from Stevenag...</td>\n",
       "      <td>None</td>\n",
       "      <td>christopher bridger 25 attacked three women af...</td>\n",
       "      <td>Christopher Bridger 25 from Stevenage Hertford...</td>\n",
       "      <td>738</td>\n",
       "      <td>60</td>\n",
       "      <td>Christopher Bridger, a paramedic from Stevenag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>fb207604ffa7e8371c622840445825db8993d4d2</td>\n",
       "      <td>Paris Saint-Germain captain Thiago Silva suffe...</td>\n",
       "      <td>Paris Saint Germain face Nice on Saturday hopi...</td>\n",
       "      <td>police say they are not to be a new york city ...</td>\n",
       "      <td>paris saint germain face nice on saturday hopi...</td>\n",
       "      <td>Thiago Silva is recovering at home from a thig...</td>\n",
       "      <td>Paris Saint Germain will face Nice on Saturday...</td>\n",
       "      <td>Paris Saint Germain, missing key players inclu...</td>\n",
       "      <td>thiago silva is recuperating at home from a th...</td>\n",
       "      <td>Thiago Silva is recuperating at home from a th...</td>\n",
       "      <td>565</td>\n",
       "      <td>40</td>\n",
       "      <td>Paris Saint Germain will face Nice on Saturday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>d25d52c434a13c1df5faa593e8a097d2f501a2b6</td>\n",
       "      <td>.50-caliber bullets equipped with optical sens...</td>\n",
       "      <td>CNN You know the phrase dodging a bullet Forge...</td>\n",
       "      <td>the man was arrested in the scene of the incid...</td>\n",
       "      <td>cnn you know the phrase dodging a bullet forge...</td>\n",
       "      <td>the smart bullets .50 caliber projectiles pass...</td>\n",
       "      <td>The US military has made significant progress ...</td>\n",
       "      <td>The U.S. military has made significant advance...</td>\n",
       "      <td>the u. s. military has made great progress in ...</td>\n",
       "      <td>In February the smart bullets .50 caliber proj...</td>\n",
       "      <td>331</td>\n",
       "      <td>39</td>\n",
       "      <td>The US military has made progress in developin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                        id  \\\n",
       "97          97  3815d19af18ff22be6ad6095722d7367bb7271af   \n",
       "98          98  fb207604ffa7e8371c622840445825db8993d4d2   \n",
       "99          99  d25d52c434a13c1df5faa593e8a097d2f501a2b6   \n",
       "\n",
       "                                           highlights  \\\n",
       "97  Christopher Bridger, 25, attacked three women ...   \n",
       "98  Paris Saint-Germain captain Thiago Silva suffe...   \n",
       "99  .50-caliber bullets equipped with optical sens...   \n",
       "\n",
       "                                              article  \\\n",
       "97  A paramedic who pretended he was gay to get cl...   \n",
       "98  Paris Saint Germain face Nice on Saturday hopi...   \n",
       "99  CNN You know the phrase dodging a bullet Forge...   \n",
       "\n",
       "                                         baseline-abs  \\\n",
       "97  the man s body was found in the hospital in th...   \n",
       "98  police say they are not to be a new york city ...   \n",
       "99  the man was arrested in the scene of the incid...   \n",
       "\n",
       "                                     bert-base-abs-zs  \\\n",
       "97  a paramedic who pretended he was gay to get cl...   \n",
       "98  paris saint germain face nice on saturday hopi...   \n",
       "99  cnn you know the phrase dodging a bullet forge...   \n",
       "\n",
       "                                      t5-small-abs-zs  \\\n",
       "97  Christopher Bridger 25, from Stevenage Hertfor...   \n",
       "98  Thiago Silva is recovering at home from a thig...   \n",
       "99  the smart bullets .50 caliber projectiles pass...   \n",
       "\n",
       "                                       gpt-3.5-abs-zs  \\\n",
       "97  Christopher Bridger, a paramedic from Stevenag...   \n",
       "98  Paris Saint Germain will face Nice on Saturday...   \n",
       "99  The US military has made significant progress ...   \n",
       "\n",
       "                                         gpt-4-abs-zs  \\\n",
       "97                                               None   \n",
       "98  Paris Saint Germain, missing key players inclu...   \n",
       "99  The U.S. military has made significant advance...   \n",
       "\n",
       "                                     bert-base-abs-ft  \\\n",
       "97  christopher bridger 25 attacked three women af...   \n",
       "98  thiago silva is recuperating at home from a th...   \n",
       "99  the u. s. military has made great progress in ...   \n",
       "\n",
       "                                      t5-small-abs-ft  article_len  \\\n",
       "97  Christopher Bridger 25 from Stevenage Hertford...          738   \n",
       "98  Thiago Silva is recuperating at home from a th...          565   \n",
       "99  In February the smart bullets .50 caliber proj...          331   \n",
       "\n",
       "    highlights_len                                     gpt-3.5-abs-ft  \n",
       "97              60  Christopher Bridger, a paramedic from Stevenag...  \n",
       "98              40  Paris Saint Germain will face Nice on Saturday...  \n",
       "99              39  The US military has made progress in developin...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.info(\"Generating GPT35 Extractive Summaries (After Prompt Tuning) ...\") \n",
    "st_gpt35_ft_ats=time.time()\n",
    "sample_test_df['gpt-3.5-abs-ft'] = sample_test_df.apply(lambda rec: generate_GPT_Abs_1_Shot(rec, 'gpt35', 0, 60), axis=1)\n",
    "end_gpt35_ft_ats=time.time()\n",
    "logging.info(f\"GPT35 FT ATS Duration - {roundTS(st_gpt35_ft_ats, end_gpt35_ft_ats)} seconds\")   \n",
    "sample_test_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa29b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration['GPT35-FT'] = avgTimePerRecord(st_gpt35_ft_ats, end_gpt35_ft_ats,NO_OF_TEST_RECORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e773867",
   "metadata": {},
   "source": [
    "#### GPT 4 Abstractive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4f4e0043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>highlights</th>\n",
       "      <th>article</th>\n",
       "      <th>baseline-abs</th>\n",
       "      <th>bert-base-abs-zs</th>\n",
       "      <th>t5-small-abs-zs</th>\n",
       "      <th>gpt-3.5-abs-zs</th>\n",
       "      <th>gpt-4-abs-zs</th>\n",
       "      <th>bert-base-abs-ft</th>\n",
       "      <th>t5-small-abs-ft</th>\n",
       "      <th>article_len</th>\n",
       "      <th>highlights_len</th>\n",
       "      <th>gpt-3.5-abs-ft</th>\n",
       "      <th>gpt-4-abs-ft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>3815d19af18ff22be6ad6095722d7367bb7271af</td>\n",
       "      <td>Christopher Bridger, 25, attacked three women ...</td>\n",
       "      <td>A paramedic who pretended he was gay to get cl...</td>\n",
       "      <td>the man s body was found in the hospital in th...</td>\n",
       "      <td>a paramedic who pretended he was gay to get cl...</td>\n",
       "      <td>Christopher Bridger 25, from Stevenage Hertfor...</td>\n",
       "      <td>Christopher Bridger, a paramedic from Stevenag...</td>\n",
       "      <td>None</td>\n",
       "      <td>christopher bridger 25 attacked three women af...</td>\n",
       "      <td>Christopher Bridger 25 from Stevenage Hertford...</td>\n",
       "      <td>738</td>\n",
       "      <td>60</td>\n",
       "      <td>Christopher Bridger, a paramedic from Stevenag...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>fb207604ffa7e8371c622840445825db8993d4d2</td>\n",
       "      <td>Paris Saint-Germain captain Thiago Silva suffe...</td>\n",
       "      <td>Paris Saint Germain face Nice on Saturday hopi...</td>\n",
       "      <td>police say they are not to be a new york city ...</td>\n",
       "      <td>paris saint germain face nice on saturday hopi...</td>\n",
       "      <td>Thiago Silva is recovering at home from a thig...</td>\n",
       "      <td>Paris Saint Germain will face Nice on Saturday...</td>\n",
       "      <td>Paris Saint Germain, missing key players inclu...</td>\n",
       "      <td>thiago silva is recuperating at home from a th...</td>\n",
       "      <td>Thiago Silva is recuperating at home from a th...</td>\n",
       "      <td>565</td>\n",
       "      <td>40</td>\n",
       "      <td>Paris Saint Germain will face Nice on Saturday...</td>\n",
       "      <td>Paris Saint Germain, missing key players inclu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>d25d52c434a13c1df5faa593e8a097d2f501a2b6</td>\n",
       "      <td>.50-caliber bullets equipped with optical sens...</td>\n",
       "      <td>CNN You know the phrase dodging a bullet Forge...</td>\n",
       "      <td>the man was arrested in the scene of the incid...</td>\n",
       "      <td>cnn you know the phrase dodging a bullet forge...</td>\n",
       "      <td>the smart bullets .50 caliber projectiles pass...</td>\n",
       "      <td>The US military has made significant progress ...</td>\n",
       "      <td>The U.S. military has made significant advance...</td>\n",
       "      <td>the u. s. military has made great progress in ...</td>\n",
       "      <td>In February the smart bullets .50 caliber proj...</td>\n",
       "      <td>331</td>\n",
       "      <td>39</td>\n",
       "      <td>The US military has made progress in developin...</td>\n",
       "      <td>The U.S. military has made significant advance...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                        id  \\\n",
       "97          97  3815d19af18ff22be6ad6095722d7367bb7271af   \n",
       "98          98  fb207604ffa7e8371c622840445825db8993d4d2   \n",
       "99          99  d25d52c434a13c1df5faa593e8a097d2f501a2b6   \n",
       "\n",
       "                                           highlights  \\\n",
       "97  Christopher Bridger, 25, attacked three women ...   \n",
       "98  Paris Saint-Germain captain Thiago Silva suffe...   \n",
       "99  .50-caliber bullets equipped with optical sens...   \n",
       "\n",
       "                                              article  \\\n",
       "97  A paramedic who pretended he was gay to get cl...   \n",
       "98  Paris Saint Germain face Nice on Saturday hopi...   \n",
       "99  CNN You know the phrase dodging a bullet Forge...   \n",
       "\n",
       "                                         baseline-abs  \\\n",
       "97  the man s body was found in the hospital in th...   \n",
       "98  police say they are not to be a new york city ...   \n",
       "99  the man was arrested in the scene of the incid...   \n",
       "\n",
       "                                     bert-base-abs-zs  \\\n",
       "97  a paramedic who pretended he was gay to get cl...   \n",
       "98  paris saint germain face nice on saturday hopi...   \n",
       "99  cnn you know the phrase dodging a bullet forge...   \n",
       "\n",
       "                                      t5-small-abs-zs  \\\n",
       "97  Christopher Bridger 25, from Stevenage Hertfor...   \n",
       "98  Thiago Silva is recovering at home from a thig...   \n",
       "99  the smart bullets .50 caliber projectiles pass...   \n",
       "\n",
       "                                       gpt-3.5-abs-zs  \\\n",
       "97  Christopher Bridger, a paramedic from Stevenag...   \n",
       "98  Paris Saint Germain will face Nice on Saturday...   \n",
       "99  The US military has made significant progress ...   \n",
       "\n",
       "                                         gpt-4-abs-zs  \\\n",
       "97                                               None   \n",
       "98  Paris Saint Germain, missing key players inclu...   \n",
       "99  The U.S. military has made significant advance...   \n",
       "\n",
       "                                     bert-base-abs-ft  \\\n",
       "97  christopher bridger 25 attacked three women af...   \n",
       "98  thiago silva is recuperating at home from a th...   \n",
       "99  the u. s. military has made great progress in ...   \n",
       "\n",
       "                                      t5-small-abs-ft  article_len  \\\n",
       "97  Christopher Bridger 25 from Stevenage Hertford...          738   \n",
       "98  Thiago Silva is recuperating at home from a th...          565   \n",
       "99  In February the smart bullets .50 caliber proj...          331   \n",
       "\n",
       "    highlights_len                                     gpt-3.5-abs-ft  \\\n",
       "97              60  Christopher Bridger, a paramedic from Stevenag...   \n",
       "98              40  Paris Saint Germain will face Nice on Saturday...   \n",
       "99              39  The US military has made progress in developin...   \n",
       "\n",
       "                                         gpt-4-abs-ft  \n",
       "97                                               None  \n",
       "98  Paris Saint Germain, missing key players inclu...  \n",
       "99  The U.S. military has made significant advance...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.info(\"Generating GPT4 Extractive Summaries (After Prompt Tuning) ...\") \n",
    "st_gpt4_ft_ats=time.time()\n",
    "sample_test_df['gpt-4-abs-ft'] = sample_test_df.apply(lambda rec: generate_GPT_Abs_1_Shot(rec, 'gpt4', 1, 60), axis=1)\n",
    "end_gpt4_ft_ats=time.time()\n",
    "logging.info(f\"GPT4 FT ETS Duration - {roundTS(st_gpt4_ft_ats, end_gpt4_ft_ats)} seconds\")   \n",
    "sample_test_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "64e407a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration['GPT4-FT'] = avgTimePerRecord(st_gpt4_ft_ats, end_gpt4_ft_ats,NO_OF_TEST_RECORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1ef858",
   "metadata": {},
   "source": [
    "#### Saving the generated summaries for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a84e3814",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time_ats=time.time()\n",
    "logging.info(f\"Total ATS Duration - {roundTS(start_time_ats, end_time_ats)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9b72fa0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Baseline': 1.6643,\n",
       " 'BERT': 9.1136,\n",
       " 'T5': 2.3688,\n",
       " 'GPT35': 1.2674,\n",
       " 'GPT4': 6.3076,\n",
       " 'BERT-FT': 0.471,\n",
       " 'T5-FT': 1.8913,\n",
       " 'GPT35-FT': 1.1947,\n",
       " 'GPT4-FT': 5.7775}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8597e054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>models</th>\n",
       "      <th>avg_inf_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>1.6643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BERT</td>\n",
       "      <td>9.1136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T5</td>\n",
       "      <td>2.3688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GPT35</td>\n",
       "      <td>1.2674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GPT4</td>\n",
       "      <td>6.3076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BERT-FT</td>\n",
       "      <td>0.4710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>T5-FT</td>\n",
       "      <td>1.8913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GPT35-FT</td>\n",
       "      <td>1.1947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GPT4-FT</td>\n",
       "      <td>5.7775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     models  avg_inf_time\n",
       "0  Baseline        1.6643\n",
       "1      BERT        9.1136\n",
       "2        T5        2.3688\n",
       "3     GPT35        1.2674\n",
       "4      GPT4        6.3076\n",
       "5   BERT-FT        0.4710\n",
       "6     T5-FT        1.8913\n",
       "7  GPT35-FT        1.1947\n",
       "8   GPT4-FT        5.7775"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_df = pd.DataFrame(duration.items(), columns=['models', 'avg_inf_time'])\n",
    "duration_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "26cfd04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"MAP - ATS Average Inference Time Per Request - {duration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "919b8b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_file_path = './output/abs-ts-duration-final.csv'\n",
    "logging.info(f\"Duration will be saved in this file : {duration_file_path}\")\n",
    "duration_df.to_csv(duration_file_path,  mode=\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "efe2ea79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries will be saved in this file :  ./output/abs-ts-final.csv\n"
     ]
    }
   ],
   "source": [
    "file_path = './output/abs-ts-final.csv'\n",
    "print(\"Summaries will be saved in this file : \", file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "412f6c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Abstractive Text Summarization End \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6f5cfc44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>highlights</th>\n",
       "      <th>article</th>\n",
       "      <th>baseline-abs</th>\n",
       "      <th>bert-base-abs-zs</th>\n",
       "      <th>t5-small-abs-zs</th>\n",
       "      <th>gpt-3.5-abs-zs</th>\n",
       "      <th>gpt-4-abs-zs</th>\n",
       "      <th>bert-base-abs-ft</th>\n",
       "      <th>t5-small-abs-ft</th>\n",
       "      <th>article_len</th>\n",
       "      <th>highlights_len</th>\n",
       "      <th>gpt-3.5-abs-ft</th>\n",
       "      <th>gpt-4-abs-ft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>92c514c913c0bdfe25341af9fd72b29db544099b</td>\n",
       "      <td>Experts question if  packed out planes are put...</td>\n",
       "      <td>Ever noticed how plane seats appear to be gett...</td>\n",
       "      <td>police say they are not to be a new police say...</td>\n",
       "      <td>ever noticed how plane seats appear to be gett...</td>\n",
       "      <td>some experts are questioning if shrinking spac...</td>\n",
       "      <td>Experts are concerned that the shrinking space...</td>\n",
       "      <td>Experts are raising concerns that the decreasi...</td>\n",
       "      <td>the dot and faa are happy to set standards for...</td>\n",
       "      <td>A U.S consumer advisory group set up by the De...</td>\n",
       "      <td>374</td>\n",
       "      <td>36</td>\n",
       "      <td>Experts are concerned that the shrinking space...</td>\n",
       "      <td>Experts are warning that the decreasing space ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2003841c7dc0e7c5b1a248f9cd536d727f27a45a</td>\n",
       "      <td>Drunk teenage boy climbed into lion enclosure ...</td>\n",
       "      <td>A drunk teenage boy had to be rescued by secur...</td>\n",
       "      <td>police say they are not to be a new york polic...</td>\n",
       "      <td>a drunk teenage boy had to be rescued by secur...</td>\n",
       "      <td>Rahul Kumar 17 climbed into the enclosure fenc...</td>\n",
       "      <td>None</td>\n",
       "      <td>Rahul Kumar, a 17-year-old intoxicated teenage...</td>\n",
       "      <td>rahul kumar 17 climbed into the enclosure fenc...</td>\n",
       "      <td>Rahul Kumar 17 climbed into a lions enclosure ...</td>\n",
       "      <td>317</td>\n",
       "      <td>38</td>\n",
       "      <td>None</td>\n",
       "      <td>A drunk teenager, Rahul Kumar, 17, jumped into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>91b7d2311527f5c2b63a65ca98d21d9c92485149</td>\n",
       "      <td>Nottingham Forest are close to extending Dougi...</td>\n",
       "      <td>Dougie Freedman is on the verge of agreeing a ...</td>\n",
       "      <td>the new york s office says the new york is the...</td>\n",
       "      <td>dougie freedman is on the verge of agreeing a ...</td>\n",
       "      <td>Dougie Freedman is set to sign a new two year ...</td>\n",
       "      <td>Dougie Freedman is close to signing a new two-...</td>\n",
       "      <td>Dougie Freedman is close to signing a new two-...</td>\n",
       "      <td>dougie freedman is set to sign a new two year ...</td>\n",
       "      <td>Dougie Freedman is set to sign a new two year ...</td>\n",
       "      <td>114</td>\n",
       "      <td>35</td>\n",
       "      <td>Nottingham Forest's manager, Dougie Freedman, ...</td>\n",
       "      <td>Dougie Freedman is close to signing a new two-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        id  \\\n",
       "0           0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
       "1           1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
       "2           2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n",
       "\n",
       "                                          highlights  \\\n",
       "0  Experts question if  packed out planes are put...   \n",
       "1  Drunk teenage boy climbed into lion enclosure ...   \n",
       "2  Nottingham Forest are close to extending Dougi...   \n",
       "\n",
       "                                             article  \\\n",
       "0  Ever noticed how plane seats appear to be gett...   \n",
       "1  A drunk teenage boy had to be rescued by secur...   \n",
       "2  Dougie Freedman is on the verge of agreeing a ...   \n",
       "\n",
       "                                        baseline-abs  \\\n",
       "0  police say they are not to be a new police say...   \n",
       "1  police say they are not to be a new york polic...   \n",
       "2  the new york s office says the new york is the...   \n",
       "\n",
       "                                    bert-base-abs-zs  \\\n",
       "0  ever noticed how plane seats appear to be gett...   \n",
       "1  a drunk teenage boy had to be rescued by secur...   \n",
       "2  dougie freedman is on the verge of agreeing a ...   \n",
       "\n",
       "                                     t5-small-abs-zs  \\\n",
       "0  some experts are questioning if shrinking spac...   \n",
       "1  Rahul Kumar 17 climbed into the enclosure fenc...   \n",
       "2  Dougie Freedman is set to sign a new two year ...   \n",
       "\n",
       "                                      gpt-3.5-abs-zs  \\\n",
       "0  Experts are concerned that the shrinking space...   \n",
       "1                                               None   \n",
       "2  Dougie Freedman is close to signing a new two-...   \n",
       "\n",
       "                                        gpt-4-abs-zs  \\\n",
       "0  Experts are raising concerns that the decreasi...   \n",
       "1  Rahul Kumar, a 17-year-old intoxicated teenage...   \n",
       "2  Dougie Freedman is close to signing a new two-...   \n",
       "\n",
       "                                    bert-base-abs-ft  \\\n",
       "0  the dot and faa are happy to set standards for...   \n",
       "1  rahul kumar 17 climbed into the enclosure fenc...   \n",
       "2  dougie freedman is set to sign a new two year ...   \n",
       "\n",
       "                                     t5-small-abs-ft  article_len  \\\n",
       "0  A U.S consumer advisory group set up by the De...          374   \n",
       "1  Rahul Kumar 17 climbed into a lions enclosure ...          317   \n",
       "2  Dougie Freedman is set to sign a new two year ...          114   \n",
       "\n",
       "   highlights_len                                     gpt-3.5-abs-ft  \\\n",
       "0              36  Experts are concerned that the shrinking space...   \n",
       "1              38                                               None   \n",
       "2              35  Nottingham Forest's manager, Dougie Freedman, ...   \n",
       "\n",
       "                                        gpt-4-abs-ft  \n",
       "0  Experts are warning that the decreasing space ...  \n",
       "1  A drunk teenager, Rahul Kumar, 17, jumped into...  \n",
       "2  Dougie Freedman is close to signing a new two-...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fe411a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_df.to_csv(file_path,  mode=\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "af534b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>highlights</th>\n",
       "      <th>article</th>\n",
       "      <th>baseline-abs</th>\n",
       "      <th>bert-base-abs-zs</th>\n",
       "      <th>t5-small-abs-zs</th>\n",
       "      <th>gpt-3.5-abs-zs</th>\n",
       "      <th>gpt-4-abs-zs</th>\n",
       "      <th>bert-base-abs-ft</th>\n",
       "      <th>t5-small-abs-ft</th>\n",
       "      <th>article_len</th>\n",
       "      <th>highlights_len</th>\n",
       "      <th>gpt-3.5-abs-ft</th>\n",
       "      <th>gpt-4-abs-ft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92c514c913c0bdfe25341af9fd72b29db544099b</td>\n",
       "      <td>Experts question if  packed out planes are put...</td>\n",
       "      <td>Ever noticed how plane seats appear to be gett...</td>\n",
       "      <td>police say they are not to be a new police say...</td>\n",
       "      <td>ever noticed how plane seats appear to be gett...</td>\n",
       "      <td>some experts are questioning if shrinking spac...</td>\n",
       "      <td>Experts are concerned that the shrinking space...</td>\n",
       "      <td>Experts are raising concerns that the decreasi...</td>\n",
       "      <td>the dot and faa are happy to set standards for...</td>\n",
       "      <td>A U.S consumer advisory group set up by the De...</td>\n",
       "      <td>374</td>\n",
       "      <td>36</td>\n",
       "      <td>Experts are concerned that the shrinking space...</td>\n",
       "      <td>Experts are warning that the decreasing space ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2003841c7dc0e7c5b1a248f9cd536d727f27a45a</td>\n",
       "      <td>Drunk teenage boy climbed into lion enclosure ...</td>\n",
       "      <td>A drunk teenage boy had to be rescued by secur...</td>\n",
       "      <td>police say they are not to be a new york polic...</td>\n",
       "      <td>a drunk teenage boy had to be rescued by secur...</td>\n",
       "      <td>Rahul Kumar 17 climbed into the enclosure fenc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rahul Kumar, a 17-year-old intoxicated teenage...</td>\n",
       "      <td>rahul kumar 17 climbed into the enclosure fenc...</td>\n",
       "      <td>Rahul Kumar 17 climbed into a lions enclosure ...</td>\n",
       "      <td>317</td>\n",
       "      <td>38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A drunk teenager, Rahul Kumar, 17, jumped into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>91b7d2311527f5c2b63a65ca98d21d9c92485149</td>\n",
       "      <td>Nottingham Forest are close to extending Dougi...</td>\n",
       "      <td>Dougie Freedman is on the verge of agreeing a ...</td>\n",
       "      <td>the new york s office says the new york is the...</td>\n",
       "      <td>dougie freedman is on the verge of agreeing a ...</td>\n",
       "      <td>Dougie Freedman is set to sign a new two year ...</td>\n",
       "      <td>Dougie Freedman is close to signing a new two-...</td>\n",
       "      <td>Dougie Freedman is close to signing a new two-...</td>\n",
       "      <td>dougie freedman is set to sign a new two year ...</td>\n",
       "      <td>Dougie Freedman is set to sign a new two year ...</td>\n",
       "      <td>114</td>\n",
       "      <td>35</td>\n",
       "      <td>Nottingham Forest's manager, Dougie Freedman, ...</td>\n",
       "      <td>Dougie Freedman is close to signing a new two-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0                                        id  \\\n",
       "0             0           0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
       "1             1           1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
       "2             2           2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n",
       "\n",
       "                                          highlights  \\\n",
       "0  Experts question if  packed out planes are put...   \n",
       "1  Drunk teenage boy climbed into lion enclosure ...   \n",
       "2  Nottingham Forest are close to extending Dougi...   \n",
       "\n",
       "                                             article  \\\n",
       "0  Ever noticed how plane seats appear to be gett...   \n",
       "1  A drunk teenage boy had to be rescued by secur...   \n",
       "2  Dougie Freedman is on the verge of agreeing a ...   \n",
       "\n",
       "                                        baseline-abs  \\\n",
       "0  police say they are not to be a new police say...   \n",
       "1  police say they are not to be a new york polic...   \n",
       "2  the new york s office says the new york is the...   \n",
       "\n",
       "                                    bert-base-abs-zs  \\\n",
       "0  ever noticed how plane seats appear to be gett...   \n",
       "1  a drunk teenage boy had to be rescued by secur...   \n",
       "2  dougie freedman is on the verge of agreeing a ...   \n",
       "\n",
       "                                     t5-small-abs-zs  \\\n",
       "0  some experts are questioning if shrinking spac...   \n",
       "1  Rahul Kumar 17 climbed into the enclosure fenc...   \n",
       "2  Dougie Freedman is set to sign a new two year ...   \n",
       "\n",
       "                                      gpt-3.5-abs-zs  \\\n",
       "0  Experts are concerned that the shrinking space...   \n",
       "1                                                NaN   \n",
       "2  Dougie Freedman is close to signing a new two-...   \n",
       "\n",
       "                                        gpt-4-abs-zs  \\\n",
       "0  Experts are raising concerns that the decreasi...   \n",
       "1  Rahul Kumar, a 17-year-old intoxicated teenage...   \n",
       "2  Dougie Freedman is close to signing a new two-...   \n",
       "\n",
       "                                    bert-base-abs-ft  \\\n",
       "0  the dot and faa are happy to set standards for...   \n",
       "1  rahul kumar 17 climbed into the enclosure fenc...   \n",
       "2  dougie freedman is set to sign a new two year ...   \n",
       "\n",
       "                                     t5-small-abs-ft  article_len  \\\n",
       "0  A U.S consumer advisory group set up by the De...          374   \n",
       "1  Rahul Kumar 17 climbed into a lions enclosure ...          317   \n",
       "2  Dougie Freedman is set to sign a new two year ...          114   \n",
       "\n",
       "   highlights_len                                     gpt-3.5-abs-ft  \\\n",
       "0              36  Experts are concerned that the shrinking space...   \n",
       "1              38                                                NaN   \n",
       "2              35  Nottingham Forest's manager, Dougie Freedman, ...   \n",
       "\n",
       "                                        gpt-4-abs-ft  \n",
       "0  Experts are warning that the decreasing space ...  \n",
       "1  A drunk teenager, Rahul Kumar, 17, jumped into...  \n",
       "2  Dougie Freedman is close to signing a new two-...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_content = pd.read_csv(file_path)\n",
    "read_content.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c0d8a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
