{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_28764\\3964258435.py:9: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge_metric = load_metric('rouge')\n",
      "C:\\Users\\Dell\\AppData\\Roaming\\Python\\Python311\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "rouge_metric = load_metric('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_OPENAI_KEY = \"[REPLACE KEY ]\"\n",
    "AZURE_OPENAI_ENDPOINT = \"https://tmap-openai.openai.azure.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = \"https://tmap-openai.openai.azure.com\", \n",
    "  api_key=\"[REPLACE KEY ]\",  \n",
    "  api_version=\"2023-09-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(client.models.list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT - ATS - Prompt Tuning Experiements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_df = pd.read_csv('./input/test.csv', nrows=100)\n",
    "sample_test_df = sample_test_df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_GPT_ATS_Summary(record, model, temp, max_tokens):\n",
    "    summary = \"\"\n",
    "    \n",
    "    abs_pmt = f\"\"\"\n",
    "                Your task is to create a concise, factual summary, \n",
    "                \n",
    "                by selecting and combining key sentences from  the original text. \n",
    "                \n",
    "                Text is delimited by triple backticks. \n",
    "\n",
    "                TEXT: ```{record['article']}```\n",
    "            \"\"\"\n",
    "     try:\n",
    "        response = client.chat.completions.create(\n",
    "        model=model, \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a LLM trained by OpenAI.\"},\n",
    "            {\"role\": \"user\", \"content\": abs_pmt}\n",
    "\n",
    "        ],\n",
    "        max_tokens = max_tokens,\n",
    "        temperature = temp,\n",
    "        n=1,        \n",
    "        )\n",
    "        summary = response.choices[0].message.content\n",
    "    except Exception:\n",
    "        print(Exception)\n",
    "        summary = \"ERROR\"\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generateAbsBaselineComplexPrompt(text):\n",
    "    summary = \"\"\n",
    "    \n",
    "    abs_pmt = f\"\"\"\n",
    "                As a LLM trained by OpenAI, create a concise and factualsummary, \n",
    "                \n",
    "                by selecting and combining key sentences from the original text,\n",
    "                \n",
    "                while adhering to these guidelines:\n",
    "                \n",
    "                1.Craft a summary that is detailed, in-depth, while maintaining clarity and conciseness.\n",
    "                \n",
    "                2.Incorporate main ideas, incorporate essential information and focus on critical aspects.\n",
    "                \n",
    "                3.Rely strictly on the provided text, without including external information.\n",
    "                \n",
    "                Text is delimited by triple backticks TEXT: ```{text}```\n",
    "                \"\"\"\n",
    "   \n",
    "    \n",
    "    #create a concise summary by selecting and combining key sentences from the original text : {text}\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "        model=\"gpt4\", # model = \"deployment_name\".\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a LLM trained by OpenAI.\"},\n",
    "            {\"role\": \"user\", \"content\": abs_pmt}\n",
    "\n",
    "        ])\n",
    "        summary = response.choices[0].message.content\n",
    "    except Exception:\n",
    "        print(Exception)\n",
    "        summary = \"ERROR\"\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n"
     ]
    }
   ],
   "source": [
    "sample_test_df['gpt4_ats_low_temp_30_words'] = sample_test_df.apply(lambda rec: generate_GPT_ATS_Summary(rec, 'gpt4', 0, 30), axis=1)\n",
    "sample_test_df['gpt4_ats_med_temp_30_words'] = sample_test_df.apply(lambda rec: generate_GPT_ATS_Summary(rec, 'gpt4', 1, 30), axis=1)\n",
    "sample_test_df['gpt4_ats_high_temp_30_words'] = sample_test_df.apply(lambda rec: generate_GPT_ATS_Summary(rec, 'gpt4', 2, 30), axis=1)\n",
    "\n",
    "sample_test_df['gpt35_ats_low_temp_30_words'] = sample_test_df.apply(lambda rec: generate_GPT_ATS_Summary(rec, 'gpt35', 0, 30), axis=1)\n",
    "sample_test_df['gpt35_ats_med_temp_30_words'] = sample_test_df.apply(lambda rec: generate_GPT_ATS_Summary(rec, 'gpt35', 1, 30), axis=1)\n",
    "sample_test_df['gpt35_ats_high_temp_30_words'] = sample_test_df.apply(lambda rec: generate_GPT_ATS_Summary(rec, 'gpt35', 2, 30), axis=1)\n",
    "\n",
    "sample_test_df['gpt4_ats_low_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_ATS_Summary(rec, 'gpt4', 0, 60), axis=1)\n",
    "sample_test_df['gpt4_ats_med_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_ATS_Summary(rec, 'gpt4', 1, 60), axis=1)\n",
    "sample_test_df['gpt4_ats_high_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_ATS_Summary(rec, 'gpt4', 2, 60), axis=1)\n",
    "\n",
    "sample_test_df['gpt35_ats_low_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_ATS_Summary(rec, 'gpt35', 0, 60), axis=1)\n",
    "sample_test_df['gpt35_ats_med_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_ATS_Summary(rec, 'gpt35', 1, 60), axis=1)\n",
    "sample_test_df['gpt35_ats_high_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_ATS_Summary(rec, 'gpt35', 2, 60), axis=1)\n",
    "\n",
    "sample_test_df['gpt4_ats_low_temp_90_words'] = sample_test_df.apply(lambda rec: generate_GPT_ATS_Summary(rec, 'gpt4', 0, 90), axis=1)\n",
    "sample_test_df['gpt4_ats_med_temp_90_words'] = sample_test_df.apply(lambda rec: generate_GPT_ATS_Summary(rec, 'gpt4', 1, 90), axis=1)\n",
    "sample_test_df['gpt4_ats_high_temp_90_words'] = sample_test_df.apply(lambda rec: generate_GPT_ATS_Summary(rec, 'gpt4', 2, 90), axis=1)\n",
    "\n",
    "sample_test_df['gpt35_ats_low_temp_90_words'] = sample_test_df.apply(lambda rec: generate_GPT_ATS_Summary(rec, 'gpt35', 0, 90), axis=1)\n",
    "sample_test_df['gpt35_ats_med_temp_90_words'] = sample_test_df.apply(lambda rec: generate_GPT_ATS_Summary(rec, 'gpt35', 1, 90), axis=1)\n",
    "sample_test_df['gpt35_ats_high_temp_90_words'] = sample_test_df.apply(lambda rec: generate_GPT_ATS_Summary(rec, 'gpt35', 2, 90), axis=1)\n",
    "\n",
    "sample_test_df['gpt4_ats_low_temp_120_words'] = sample_test_df.apply(lambda rec: generate_GPT_ATS_Summary(rec, 'gpt4', 0, 120), axis=1)\n",
    "sample_test_df['gpt4_ats_med_temp_120_words'] = sample_test_df.apply(lambda rec: generate_GPT_ATS_Summary(rec, 'gpt4', 1, 120), axis=1)\n",
    "sample_test_df['gpt4_ats_high_temp_120_words'] = sample_test_df.apply(lambda rec: generate_GPT_ATS_Summary(rec, 'gpt4', 2, 120), axis=1)\n",
    "\n",
    "sample_test_df['gpt35_ats_low_temp_120_words'] = sample_test_df.apply(lambda rec: generate_GPT_ATS_Summary(rec, 'gpt35', 0, 120), axis=1)\n",
    "sample_test_df['gpt35_ats_med_temp_120_words'] = sample_test_df.apply(lambda rec: generate_GPT_ATS_Summary(rec, 'gpt35', 1, 120), axis=1)\n",
    "sample_test_df['gpt35_ats_high_temp_120_words'] = sample_test_df.apply(lambda rec: generate_GPT_ATS_Summary(rec, 'gpt35', 2, 120), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge_dict  {'rouge1': 0.4053058216654385, 'rouge2': 0.1107011070110701, 'rougeL': 0.210759027266028, 'rougeLsum': 0.210759027266028}\n",
      "rouge_dict  {'rouge1': 0.37278106508875747, 'rouge2': 0.08592592592592592, 'rougeL': 0.18491124260355035, 'rougeLsum': 0.18491124260355035}\n",
      "rouge_dict  {'rouge1': 0.2718589272593681, 'rouge2': 0.05003679175864606, 'rougeL': 0.13225569434239529, 'rougeLsum': 0.13225569434239529}\n",
      "rouge_dict  {'rouge1': 0.41249092229484385, 'rouge2': 0.1090909090909091, 'rougeL': 0.19607843137254902, 'rougeLsum': 0.19607843137254902}\n",
      "rouge_dict  {'rouge1': 0.3932002956393201, 'rouge2': 0.08882309400444115, 'rougeL': 0.17294900221729487, 'rougeLsum': 0.17294900221729487}\n",
      "rouge_dict  {'rouge1': 0.11596385542168675, 'rouge2': 0.016591251885369532, 'rougeL': 0.05722891566265059, 'rougeLsum': 0.05722891566265059}\n",
      "rouge_dict  {'rouge1': 0.444179104477612, 'rouge2': 0.1123729826658697, 'rougeL': 0.19940298507462687, 'rougeLsum': 0.19940298507462687}\n",
      "rouge_dict  {'rouge1': 0.4473372781065089, 'rouge2': 0.10781990521327013, 'rougeL': 0.18224852071005918, 'rougeLsum': 0.18224852071005918}\n",
      "rouge_dict  {'rouge1': 0.187683284457478, 'rouge2': 0.032883147386964184, 'rougeL': 0.08914956011730205, 'rougeLsum': 0.08914956011730205}\n",
      "rouge_dict  {'rouge1': 0.5025699600228442, 'rouge2': 0.14979988564894228, 'rougeL': 0.21016561964591662, 'rougeLsum': 0.21016561964591662}\n",
      "rouge_dict  {'rouge1': 0.47704367301231804, 'rouge2': 0.1210762331838565, 'rougeL': 0.19708846584546474, 'rougeLsum': 0.19708846584546474}\n",
      "rouge_dict  {'rouge1': 0.1243463102847182, 'rouge2': 0.006980802792321116, 'rougeL': 0.046484601975595584, 'rougeLsum': 0.046484601975595584}\n",
      "rouge_dict  {'rouge1': 0.47930082796688134, 'rouge2': 0.141804788213628, 'rougeL': 0.2097516099356026, 'rougeLsum': 0.2097516099356026}\n",
      "rouge_dict  {'rouge1': 0.4451733833177132, 'rouge2': 0.11632270168855535, 'rougeL': 0.1949390815370197, 'rougeLsum': 0.1949390815370197}\n",
      "rouge_dict  {'rouge1': 0.214859437751004, 'rouge2': 0.04020100502512563, 'rougeL': 0.09036144578313253, 'rougeLsum': 0.09036144578313253}\n",
      "rouge_dict  {'rouge1': 0.48696461824953446, 'rouge2': 0.16309412861137, 'rougeL': 0.22532588454376165, 'rougeLsum': 0.22532588454376165}\n",
      "rouge_dict  {'rouge1': 0.45919778699861685, 'rouge2': 0.11536686663590216, 'rougeL': 0.20101429230059936, 'rougeLsum': 0.20101429230059936}\n",
      "rouge_dict  {'rouge1': 0.13489736070381234, 'rouge2': 0.016634050880626222, 'rougeL': 0.05767350928641251, 'rougeLsum': 0.05767350928641251}\n",
      "rouge_dict  {'rouge1': 0.42480620155038756, 'rouge2': 0.12024825446082237, 'rougeL': 0.19612403100775197, 'rougeLsum': 0.19612403100775197}\n",
      "rouge_dict  {'rouge1': 0.4241019698725376, 'rouge2': 0.11828372632392734, 'rougeL': 0.1869447663190421, 'rougeLsum': 0.1869447663190421}\n",
      "rouge_dict  {'rouge1': 0.20396600566572237, 'rouge2': 0.03564196030781693, 'rougeL': 0.08093889113719142, 'rougeLsum': 0.08093889113719142}\n",
      "rouge_dict  {'rouge1': 0.45516141889198886, 'rouge2': 0.16354208216992422, 'rougeL': 0.21761658031088085, 'rougeLsum': 0.21761658031088085}\n",
      "rouge_dict  {'rouge1': 0.44683301343570053, 'rouge2': 0.145985401459854, 'rougeL': 0.20652591170825338, 'rougeLsum': 0.20652591170825338}\n",
      "rouge_dict  {'rouge1': 0.14950166112956811, 'rouge2': 0.014131338320864505, 'rougeL': 0.050664451827242524, 'rougeLsum': 0.050664451827242524}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt4_ats_low_temp_30_words</th>\n",
       "      <td>0.405306</td>\n",
       "      <td>0.110701</td>\n",
       "      <td>0.210759</td>\n",
       "      <td>0.210759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4_ats_med_temp_30_words</th>\n",
       "      <td>0.372781</td>\n",
       "      <td>0.085926</td>\n",
       "      <td>0.184911</td>\n",
       "      <td>0.184911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4_ats_high_temp_30_words</th>\n",
       "      <td>0.271859</td>\n",
       "      <td>0.050037</td>\n",
       "      <td>0.132256</td>\n",
       "      <td>0.132256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35_ats_low_temp_30_words</th>\n",
       "      <td>0.412491</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.196078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35_ats_med_temp_30_words</th>\n",
       "      <td>0.393200</td>\n",
       "      <td>0.088823</td>\n",
       "      <td>0.172949</td>\n",
       "      <td>0.172949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35_ats_high_temp_30_words</th>\n",
       "      <td>0.115964</td>\n",
       "      <td>0.016591</td>\n",
       "      <td>0.057229</td>\n",
       "      <td>0.057229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4_ats_low_temp_60_words</th>\n",
       "      <td>0.444179</td>\n",
       "      <td>0.112373</td>\n",
       "      <td>0.199403</td>\n",
       "      <td>0.199403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4_ats_med_temp_60_words</th>\n",
       "      <td>0.447337</td>\n",
       "      <td>0.107820</td>\n",
       "      <td>0.182249</td>\n",
       "      <td>0.182249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4_ats_high_temp_60_words</th>\n",
       "      <td>0.187683</td>\n",
       "      <td>0.032883</td>\n",
       "      <td>0.089150</td>\n",
       "      <td>0.089150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35_ats_low_temp_60_words</th>\n",
       "      <td>0.502570</td>\n",
       "      <td>0.149800</td>\n",
       "      <td>0.210166</td>\n",
       "      <td>0.210166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35_ats_med_temp_60_words</th>\n",
       "      <td>0.477044</td>\n",
       "      <td>0.121076</td>\n",
       "      <td>0.197088</td>\n",
       "      <td>0.197088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35_ats_high_temp_60_words</th>\n",
       "      <td>0.124346</td>\n",
       "      <td>0.006981</td>\n",
       "      <td>0.046485</td>\n",
       "      <td>0.046485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4_ats_low_temp_90_words</th>\n",
       "      <td>0.479301</td>\n",
       "      <td>0.141805</td>\n",
       "      <td>0.209752</td>\n",
       "      <td>0.209752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4_ats_med_temp_90_words</th>\n",
       "      <td>0.445173</td>\n",
       "      <td>0.116323</td>\n",
       "      <td>0.194939</td>\n",
       "      <td>0.194939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4_ats_high_temp_90_words</th>\n",
       "      <td>0.214859</td>\n",
       "      <td>0.040201</td>\n",
       "      <td>0.090361</td>\n",
       "      <td>0.090361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35_ats_low_temp_90_words</th>\n",
       "      <td>0.486965</td>\n",
       "      <td>0.163094</td>\n",
       "      <td>0.225326</td>\n",
       "      <td>0.225326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35_ats_med_temp_90_words</th>\n",
       "      <td>0.459198</td>\n",
       "      <td>0.115367</td>\n",
       "      <td>0.201014</td>\n",
       "      <td>0.201014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35_ats_high_temp_90_words</th>\n",
       "      <td>0.134897</td>\n",
       "      <td>0.016634</td>\n",
       "      <td>0.057674</td>\n",
       "      <td>0.057674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4_ats_low_temp_120_words</th>\n",
       "      <td>0.424806</td>\n",
       "      <td>0.120248</td>\n",
       "      <td>0.196124</td>\n",
       "      <td>0.196124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4_ats_med_temp_120_words</th>\n",
       "      <td>0.424102</td>\n",
       "      <td>0.118284</td>\n",
       "      <td>0.186945</td>\n",
       "      <td>0.186945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4_ats_high_temp_120_words</th>\n",
       "      <td>0.203966</td>\n",
       "      <td>0.035642</td>\n",
       "      <td>0.080939</td>\n",
       "      <td>0.080939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35_ats_low_temp_120_words</th>\n",
       "      <td>0.455161</td>\n",
       "      <td>0.163542</td>\n",
       "      <td>0.217617</td>\n",
       "      <td>0.217617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35_ats_med_temp_120_words</th>\n",
       "      <td>0.446833</td>\n",
       "      <td>0.145985</td>\n",
       "      <td>0.206526</td>\n",
       "      <td>0.206526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35_ats_high_temp_120_words</th>\n",
       "      <td>0.149502</td>\n",
       "      <td>0.014131</td>\n",
       "      <td>0.050664</td>\n",
       "      <td>0.050664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 rouge1    rouge2    rougeL  rougeLsum\n",
       "gpt4_ats_low_temp_30_words     0.405306  0.110701  0.210759   0.210759\n",
       "gpt4_ats_med_temp_30_words     0.372781  0.085926  0.184911   0.184911\n",
       "gpt4_ats_high_temp_30_words    0.271859  0.050037  0.132256   0.132256\n",
       "gpt35_ats_low_temp_30_words    0.412491  0.109091  0.196078   0.196078\n",
       "gpt35_ats_med_temp_30_words    0.393200  0.088823  0.172949   0.172949\n",
       "gpt35_ats_high_temp_30_words   0.115964  0.016591  0.057229   0.057229\n",
       "gpt4_ats_low_temp_60_words     0.444179  0.112373  0.199403   0.199403\n",
       "gpt4_ats_med_temp_60_words     0.447337  0.107820  0.182249   0.182249\n",
       "gpt4_ats_high_temp_60_words    0.187683  0.032883  0.089150   0.089150\n",
       "gpt35_ats_low_temp_60_words    0.502570  0.149800  0.210166   0.210166\n",
       "gpt35_ats_med_temp_60_words    0.477044  0.121076  0.197088   0.197088\n",
       "gpt35_ats_high_temp_60_words   0.124346  0.006981  0.046485   0.046485\n",
       "gpt4_ats_low_temp_90_words     0.479301  0.141805  0.209752   0.209752\n",
       "gpt4_ats_med_temp_90_words     0.445173  0.116323  0.194939   0.194939\n",
       "gpt4_ats_high_temp_90_words    0.214859  0.040201  0.090361   0.090361\n",
       "gpt35_ats_low_temp_90_words    0.486965  0.163094  0.225326   0.225326\n",
       "gpt35_ats_med_temp_90_words    0.459198  0.115367  0.201014   0.201014\n",
       "gpt35_ats_high_temp_90_words   0.134897  0.016634  0.057674   0.057674\n",
       "gpt4_ats_low_temp_120_words    0.424806  0.120248  0.196124   0.196124\n",
       "gpt4_ats_med_temp_120_words    0.424102  0.118284  0.186945   0.186945\n",
       "gpt4_ats_high_temp_120_words   0.203966  0.035642  0.080939   0.080939\n",
       "gpt35_ats_low_temp_120_words   0.455161  0.163542  0.217617   0.217617\n",
       "gpt35_ats_med_temp_120_words   0.446833  0.145985  0.206526   0.206526\n",
       "gpt35_ats_high_temp_120_words  0.149502  0.014131  0.050664   0.050664"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "\n",
    "models = [\n",
    "          \n",
    "          \"gpt4_ats_low_temp_30_words\",  \"gpt4_ats_med_temp_30_words\",  \"gpt4_ats_high_temp_30_words\",\n",
    "          \"gpt35_ats_low_temp_30_words\",  \"gpt35_ats_med_temp_30_words\",  \"gpt35_ats_high_temp_30_words\",\n",
    "          \n",
    "          \"gpt4_ats_low_temp_60_words\",  \"gpt4_ats_med_temp_60_words\",  \"gpt4_ats_high_temp_60_words\",\n",
    "          \"gpt35_ats_low_temp_60_words\",  \"gpt35_ats_med_temp_60_words\",  \"gpt35_ats_high_temp_60_words\",\n",
    "          \n",
    "          \"gpt4_ats_low_temp_90_words\",  \"gpt4_ats_med_temp_90_words\",  \"gpt4_ats_high_temp_90_words\",\n",
    "          \"gpt35_ats_low_temp_90_words\",  \"gpt35_ats_med_temp_90_words\",  \"gpt35_ats_high_temp_90_words\",\n",
    "          \n",
    "          \"gpt4_ats_low_temp_120_words\",  \"gpt4_ats_med_temp_120_words\",  \"gpt4_ats_high_temp_120_words\",\n",
    "          \"gpt35_ats_low_temp_120_words\",  \"gpt35_ats_med_temp_120_words\",  \"gpt35_ats_high_temp_120_words\",\n",
    "         ]\n",
    "\n",
    "\n",
    "records = []\n",
    "\n",
    "for model_name in models:\n",
    "    rouge_metric.add(prediction = sample_test_df[model_name], reference = sample_test_df['highlights'])\n",
    "    score = rouge_metric.compute()\n",
    "    rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n",
    "    print('rouge_dict ', rouge_dict )\n",
    "    records.append(rouge_dict)\n",
    "\n",
    "metrics_df = pd.DataFrame.from_records(records, index = models )\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv('./output/GPT_ATS_PT_Metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT - ATS - Few Shots Inferencing Experiements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_df = pd.read_csv('./input/test.csv', nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computation of Length of article and hughlights\n",
    "def article_len(row):\n",
    "    return len(row['article'].split())\n",
    "\n",
    "def highlights_len(row):\n",
    "    return len(row['highlights'].split())\n",
    "\n",
    "\n",
    "sample_test_df['article_len'] = sample_test_df.apply(lambda r: article_len(r), axis= 1)\n",
    "sample_test_df['highlights_len'] = sample_test_df.apply(lambda r: highlights_len(r), axis= 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_article_size_df = sample_test_df.sort_values('article_len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_df = sample_test_df[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_GPT_Abs_1_Shot(record, model, temp, max_tokens):\n",
    "    summary = \"\"\n",
    "    abs_pmt = f\"\"\"\n",
    "                Your task is to create a concise, factual summary, \n",
    "                \n",
    "                by selecting and combining key sentences from  the original text. \n",
    "                \n",
    "                Text is delimited by triple backticks. \n",
    "\n",
    "                TEXT: ```{record['article']}```\n",
    "            \"\"\"\n",
    "    \n",
    "    #create a concise summary by selecting and combining key sentences from the original text : {text}\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "        model=model, # model = \"deployment_name\".\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a LLM trained by OpenAI.\"},\n",
    "            {\"role\":\"user\",\"content\":sorted_by_article_size_df.iloc[0]['article']},\n",
    "            {\"role\":\"assistant\",\"content\":sorted_by_article_size_df.iloc[0]['highlights']},\n",
    "            {\"role\": \"user\", \"content\": abs_pmt},\n",
    "            ],\n",
    "        max_tokens = max_tokens,\n",
    "        temperature = temp, #top_p=.9\n",
    "        n=1,\n",
    "        )\n",
    "        summary = response.choices[0].message.content\n",
    "    except Exception:\n",
    "        print(Exception)\n",
    "        summary = \"ERROR\"\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def generate_GPT_Abs_3_Shots(record, model, temp, max_tokens):\n",
    "    summary = \"\"\n",
    "    \n",
    "    abs_pmt = f\"\"\"\n",
    "                Your task is to create a concise, factual summary, \n",
    "                \n",
    "                by selecting and combining key sentences from  the original text. \n",
    "                \n",
    "                Text is delimited by triple backticks. \n",
    "\n",
    "                TEXT: ```{record['article']}```\n",
    "            \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "        model=model, \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a LLM trained by OpenAI.\"},\n",
    "\n",
    "            {\"role\":\"user\",\"content\":sorted_by_article_size_df.iloc[0]['article']},\n",
    "            {\"role\":\"assistant\",\"content\":sorted_by_article_size_df.iloc[0]['highlights']},\n",
    "            \n",
    "            {\"role\":\"user\",\"content\":sorted_by_article_size_df.iloc[1]['article']},\n",
    "            {\"role\":\"assistant\",\"content\":sorted_by_article_size_df.iloc[1]['highlights']},\n",
    "\n",
    "            {\"role\":\"user\",\"content\":sorted_by_article_size_df.iloc[2]['article']},\n",
    "            {\"role\":\"assistant\",\"content\":sorted_by_article_size_df.iloc[2]['highlights']},\n",
    "            \n",
    "            {\"role\": \"user\", \"content\": abs_pmt},\n",
    "            \n",
    "        ],\n",
    "        max_tokens = max_tokens,\n",
    "        temperature = temp,\n",
    "        n=1,\n",
    "        )\n",
    "        summary = response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        summary = \"ERROR\"\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def generate_GPT_Abs_5_Shots(record, model, temp, max_tokens):\n",
    "    summary = \"\"\n",
    "       \n",
    "    abs_pmt = f\"\"\"\n",
    "                Your task is to create a concise, factual summary, \n",
    "\n",
    "                by selecting and combining key sentences from  the original text. \n",
    "\n",
    "                Text is delimited by triple backticks. \n",
    "\n",
    "                TEXT: ```{record['article']}```\n",
    "            \"\"\"\n",
    "    #print('ARTICLE ', record['article'][:200])\n",
    "        #create a concise summary by selecting and combining key sentences from the original text : {text}\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model= model, # model = \"deployment_name\".\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a LLM trained by OpenAI.\"},\n",
    "\n",
    "                {\"role\":\"user\",\"content\":sorted_by_article_size_df.iloc[0]['article']},\n",
    "                {\"role\":\"assistant\",\"content\":sorted_by_article_size_df.iloc[0]['highlights']},\n",
    "\n",
    "                {\"role\":\"user\",\"content\":sorted_by_article_size_df.iloc[1]['article']},\n",
    "                {\"role\":\"assistant\",\"content\":sorted_by_article_size_df.iloc[1]['highlights']},\n",
    "\n",
    "                {\"role\":\"user\",\"content\":sorted_by_article_size_df.iloc[2]['article']},\n",
    "                {\"role\":\"assistant\",\"content\":sorted_by_article_size_df.iloc[2]['highlights']},\n",
    "\n",
    "                {\"role\":\"user\",\"content\":sorted_by_article_size_df.iloc[3]['article']},\n",
    "                {\"role\":\"assistant\",\"content\":sorted_by_article_size_df.iloc[3]['highlights']},\n",
    "\n",
    "                {\"role\":\"user\",\"content\":sorted_by_article_size_df.iloc[4]['article']},\n",
    "                {\"role\":\"assistant\",\"content\":sorted_by_article_size_df.iloc[4]['highlights']},\n",
    "\n",
    "                {\"role\": \"user\", \"content\": abs_pmt},\n",
    "\n",
    "            ],\n",
    "            max_tokens = max_tokens,\n",
    "            temperature = temp, #top_p=.9\n",
    "            n=1,\n",
    "            )\n",
    "        #print(\"Summary - \", response.choices[0].message.content)\n",
    "        summary = response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        summary = \"ERROR\"\n",
    "  \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_GPT_Abs_5_Shots(sample_test_df.iloc[0], 'gpt4', 2, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Apply the function to each row of the dataframe\n",
    "sample_test_df['gpt4-abs-5_shots_low_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_Abs_5_Shots(rec, 'gpt4', 0, 60), axis=1)\n",
    "sample_test_df['gpt4-abs-5_shots_med_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_Abs_5_Shots(rec, 'gpt4', 1, 60), axis=1)\n",
    "sample_test_df['gpt4-abs-5_shots_high_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_Abs_5_Shots(rec, 'gpt4', 2, 60), axis=1)\n",
    "\n",
    "sample_test_df['gpt35-abs-5_shots_low_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_Abs_5_Shots(rec, 'gpt35', 0, 60), axis=1)\n",
    "sample_test_df['gpt35-abs-5_shots_med_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_Abs_5_Shots(rec, 'gpt35', 1, 60), axis=1)\n",
    "sample_test_df['gpt35-abs-5_shots_high_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_Abs_5_Shots(rec, 'gpt35', 2, 60), axis=1)\n",
    "\n",
    "sample_test_df['gpt4-abs-3_shots_low_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_Abs_3_Shots(rec, 'gpt4', 0, 60), axis=1)\n",
    "sample_test_df['gpt4-abs-3_shots_med_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_Abs_3_Shots(rec, 'gpt4', 1, 60), axis=1)\n",
    "sample_test_df['gpt4-abs-3_shots_high_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_Abs_3_Shots(rec, 'gpt4', 2, 60), axis=1)\n",
    "\n",
    "sample_test_df['gpt35-abs-3_shots_low_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_Abs_3_Shots(rec, 'gpt35', 0, 60), axis=1)\n",
    "sample_test_df['gpt35-abs-3_shots_med_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_Abs_3_Shots(rec, 'gpt35', 1, 60), axis=1)\n",
    "sample_test_df['gpt35-abs-3_shots_high_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_Abs_3_Shots(rec, 'gpt35', 2, 60), axis=1)\n",
    "\n",
    "sample_test_df['gpt4-abs-1_shot_low_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_Abs_1_Shot(rec, 'gpt4', 0, 60), axis=1)\n",
    "sample_test_df['gpt4-abs-1_shot_med_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_Abs_1_Shot(rec, 'gpt4', 1, 60), axis=1)\n",
    "sample_test_df['gpt4-abs-1_shot_high_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_Abs_1_Shot(rec, 'gpt4', 2, 60), axis=1)\n",
    "\n",
    "sample_test_df['gpt35-abs-1_shot_low_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_Abs_1_Shot(rec, 'gpt35', 0, 60), axis=1)\n",
    "sample_test_df['gpt35-abs-1_shot_med_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_Abs_1_Shot(rec, 'gpt35', 1, 60), axis=1)\n",
    "sample_test_df['gpt35-abs-1_shot_high_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_Abs_1_Shot(rec, 'gpt35', 2, 60), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge_dict  {'rouge1': 0.48203939745075325, 'rouge2': 0.14849187935034802, 'rougeL': 0.22363847045191196, 'rougeLsum': 0.22363847045191196}\n",
      "rouge_dict  {'rouge1': 0.47449584816132867, 'rouge2': 0.14489311163895488, 'rougeL': 0.21352313167259784, 'rougeLsum': 0.21352313167259784}\n",
      "rouge_dict  {'rouge1': 0.2779069767441861, 'rouge2': 0.0710128055878929, 'rougeL': 0.13488372093023257, 'rougeLsum': 0.13488372093023257}\n",
      "rouge_dict  {'rouge1': 0.5122646891043925, 'rouge2': 0.1850371216447744, 'rougeL': 0.257843696520251, 'rougeLsum': 0.257843696520251}\n",
      "rouge_dict  {'rouge1': 0.5200892857142857, 'rouge2': 0.13743016759776536, 'rougeL': 0.23325892857142855, 'rougeLsum': 0.23325892857142855}\n",
      "rouge_dict  {'rouge1': 0.07082833133253301, 'rouge2': 0.007211538461538461, 'rougeL': 0.03361344537815126, 'rougeLsum': 0.03361344537815126}\n",
      "rouge_dict  {'rouge1': 0.47884057971014493, 'rouge2': 0.15786419036564134, 'rougeL': 0.22724637681159424, 'rougeLsum': 0.22724637681159424}\n",
      "rouge_dict  {'rouge1': 0.4933920704845815, 'rouge2': 0.1730981256890849, 'rougeL': 0.24008810572687225, 'rougeLsum': 0.24008810572687225}\n",
      "rouge_dict  {'rouge1': 0.26850828729281767, 'rouge2': 0.0674778761061947, 'rougeL': 0.11933701657458565, 'rougeLsum': 0.11933701657458565}\n",
      "rouge_dict  {'rouge1': 0.5315879339783722, 'rouge2': 0.18461538461538463, 'rougeL': 0.26067159931701767, 'rougeLsum': 0.26067159931701767}\n",
      "rouge_dict  {'rouge1': 0.49612403100775193, 'rouge2': 0.14966740576496673, 'rougeL': 0.2292358803986711, 'rougeLsum': 0.2292358803986711}\n",
      "rouge_dict  {'rouge1': 0.10496292070735881, 'rouge2': 0.011422044545973729, 'rougeL': 0.04449515116942385, 'rougeLsum': 0.04449515116942385}\n",
      "rouge_dict  {'rouge1': 0.46500867553499137, 'rouge2': 0.1493920092646207, 'rougeL': 0.2197802197802198, 'rougeLsum': 0.2197802197802198}\n",
      "rouge_dict  {'rouge1': 0.49798734905117886, 'rouge2': 0.1669545192861255, 'rougeL': 0.23691776883266244, 'rougeLsum': 0.23691776883266244}\n",
      "rouge_dict  {'rouge1': 0.27852545348156815, 'rouge2': 0.07147041593438783, 'rougeL': 0.13926272674078408, 'rougeLsum': 0.13926272674078408}\n",
      "rouge_dict  {'rouge1': 0.5448504983388704, 'rouge2': 0.19068736141906875, 'rougeL': 0.27574750830564787, 'rougeLsum': 0.27574750830564787}\n",
      "rouge_dict  {'rouge1': 0.5057979017117613, 'rouge2': 0.15699281370923163, 'rougeL': 0.25289895085588066, 'rougeLsum': 0.25289895085588066}\n",
      "rouge_dict  {'rouge1': 0.12162937464142284, 'rouge2': 0.01838024124066628, 'rougeL': 0.05393000573723466, 'rougeLsum': 0.05393000573723466}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt4-abs-5_shots_low_temp_60_words</th>\n",
       "      <td>0.482039</td>\n",
       "      <td>0.148492</td>\n",
       "      <td>0.223638</td>\n",
       "      <td>0.223638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4-abs-5_shots_med_temp_60_words</th>\n",
       "      <td>0.474496</td>\n",
       "      <td>0.144893</td>\n",
       "      <td>0.213523</td>\n",
       "      <td>0.213523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4-abs-5_shots_high_temp_60_words</th>\n",
       "      <td>0.277907</td>\n",
       "      <td>0.071013</td>\n",
       "      <td>0.134884</td>\n",
       "      <td>0.134884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-abs-5_shots_low_temp_60_words</th>\n",
       "      <td>0.512265</td>\n",
       "      <td>0.185037</td>\n",
       "      <td>0.257844</td>\n",
       "      <td>0.257844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-abs-5_shots_med_temp_60_words</th>\n",
       "      <td>0.520089</td>\n",
       "      <td>0.137430</td>\n",
       "      <td>0.233259</td>\n",
       "      <td>0.233259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-abs-5_shots_high_temp_60_words</th>\n",
       "      <td>0.070828</td>\n",
       "      <td>0.007212</td>\n",
       "      <td>0.033613</td>\n",
       "      <td>0.033613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4-abs-3_shots_low_temp_60_words</th>\n",
       "      <td>0.478841</td>\n",
       "      <td>0.157864</td>\n",
       "      <td>0.227246</td>\n",
       "      <td>0.227246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4-abs-3_shots_med_temp_60_words</th>\n",
       "      <td>0.493392</td>\n",
       "      <td>0.173098</td>\n",
       "      <td>0.240088</td>\n",
       "      <td>0.240088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4-abs-3_shots_high_temp_60_words</th>\n",
       "      <td>0.268508</td>\n",
       "      <td>0.067478</td>\n",
       "      <td>0.119337</td>\n",
       "      <td>0.119337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-abs-3_shots_low_temp_60_words</th>\n",
       "      <td>0.531588</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>0.260672</td>\n",
       "      <td>0.260672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-abs-3_shots_med_temp_60_words</th>\n",
       "      <td>0.496124</td>\n",
       "      <td>0.149667</td>\n",
       "      <td>0.229236</td>\n",
       "      <td>0.229236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-abs-3_shots_high_temp_60_words</th>\n",
       "      <td>0.104963</td>\n",
       "      <td>0.011422</td>\n",
       "      <td>0.044495</td>\n",
       "      <td>0.044495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4-abs-1_shot_low_temp_60_words</th>\n",
       "      <td>0.465009</td>\n",
       "      <td>0.149392</td>\n",
       "      <td>0.219780</td>\n",
       "      <td>0.219780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4-abs-1_shot_med_temp_60_words</th>\n",
       "      <td>0.497987</td>\n",
       "      <td>0.166955</td>\n",
       "      <td>0.236918</td>\n",
       "      <td>0.236918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4-abs-1_shot_high_temp_60_words</th>\n",
       "      <td>0.278525</td>\n",
       "      <td>0.071470</td>\n",
       "      <td>0.139263</td>\n",
       "      <td>0.139263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-abs-1_shot_low_temp_60_words</th>\n",
       "      <td>0.544850</td>\n",
       "      <td>0.190687</td>\n",
       "      <td>0.275748</td>\n",
       "      <td>0.275748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-abs-1_shot_med_temp_60_words</th>\n",
       "      <td>0.505798</td>\n",
       "      <td>0.156993</td>\n",
       "      <td>0.252899</td>\n",
       "      <td>0.252899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-abs-1_shot_high_temp_60_words</th>\n",
       "      <td>0.121629</td>\n",
       "      <td>0.018380</td>\n",
       "      <td>0.053930</td>\n",
       "      <td>0.053930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        rouge1    rouge2    rougeL  rougeLsum\n",
       "gpt4-abs-5_shots_low_temp_60_words    0.482039  0.148492  0.223638   0.223638\n",
       "gpt4-abs-5_shots_med_temp_60_words    0.474496  0.144893  0.213523   0.213523\n",
       "gpt4-abs-5_shots_high_temp_60_words   0.277907  0.071013  0.134884   0.134884\n",
       "gpt35-abs-5_shots_low_temp_60_words   0.512265  0.185037  0.257844   0.257844\n",
       "gpt35-abs-5_shots_med_temp_60_words   0.520089  0.137430  0.233259   0.233259\n",
       "gpt35-abs-5_shots_high_temp_60_words  0.070828  0.007212  0.033613   0.033613\n",
       "gpt4-abs-3_shots_low_temp_60_words    0.478841  0.157864  0.227246   0.227246\n",
       "gpt4-abs-3_shots_med_temp_60_words    0.493392  0.173098  0.240088   0.240088\n",
       "gpt4-abs-3_shots_high_temp_60_words   0.268508  0.067478  0.119337   0.119337\n",
       "gpt35-abs-3_shots_low_temp_60_words   0.531588  0.184615  0.260672   0.260672\n",
       "gpt35-abs-3_shots_med_temp_60_words   0.496124  0.149667  0.229236   0.229236\n",
       "gpt35-abs-3_shots_high_temp_60_words  0.104963  0.011422  0.044495   0.044495\n",
       "gpt4-abs-1_shot_low_temp_60_words     0.465009  0.149392  0.219780   0.219780\n",
       "gpt4-abs-1_shot_med_temp_60_words     0.497987  0.166955  0.236918   0.236918\n",
       "gpt4-abs-1_shot_high_temp_60_words    0.278525  0.071470  0.139263   0.139263\n",
       "gpt35-abs-1_shot_low_temp_60_words    0.544850  0.190687  0.275748   0.275748\n",
       "gpt35-abs-1_shot_med_temp_60_words    0.505798  0.156993  0.252899   0.252899\n",
       "gpt35-abs-1_shot_high_temp_60_words   0.121629  0.018380  0.053930   0.053930"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "models = [\n",
    "\n",
    "          'gpt4-abs-5_shots_low_temp_60_words', \n",
    "          'gpt4-abs-5_shots_med_temp_60_words' , \n",
    "          'gpt4-abs-5_shots_high_temp_60_words',\n",
    "          'gpt35-abs-5_shots_low_temp_60_words', \n",
    "          'gpt35-abs-5_shots_med_temp_60_words', \n",
    "          'gpt35-abs-5_shots_high_temp_60_words',\n",
    "\n",
    "          'gpt4-abs-3_shots_low_temp_60_words', \n",
    "          'gpt4-abs-3_shots_med_temp_60_words' , \n",
    "          'gpt4-abs-3_shots_high_temp_60_words',\n",
    "          'gpt35-abs-3_shots_low_temp_60_words', \n",
    "          'gpt35-abs-3_shots_med_temp_60_words', \n",
    "          'gpt35-abs-3_shots_high_temp_60_words',\n",
    "    \n",
    "          'gpt4-abs-1_shot_low_temp_60_words', \n",
    "          'gpt4-abs-1_shot_med_temp_60_words' , \n",
    "          'gpt4-abs-1_shot_high_temp_60_words',\n",
    "          'gpt35-abs-1_shot_low_temp_60_words', \n",
    "          'gpt35-abs-1_shot_med_temp_60_words', \n",
    "          'gpt35-abs-1_shot_high_temp_60_words',\n",
    "         ]\n",
    "\n",
    "records = []\n",
    "\n",
    "for model_name in models:\n",
    "    rouge_metric.add(prediction = sample_test_df[model_name], reference = sample_test_df['highlights'])\n",
    "    score = rouge_metric.compute()\n",
    "    rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n",
    "    print('rouge_dict ', rouge_dict )\n",
    "    records.append(rouge_dict)\n",
    "\n",
    "metrics_df = pd.DataFrame.from_records(records, index = models )\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv(\"./output/GPT_ATS_FS_Metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT ETS Prompt Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_df = pd.read_csv('./input/test.csv', nrows=100)\n",
    "sample_test_df = sample_test_df[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_GPT_Ext_Summary(record, model, temp, max_tokens):\n",
    "    summary = \"\"\n",
    "    \n",
    "    ext_pmt = f\"\"\"\n",
    "                Your task is to Generate extractive summary from the original text, \n",
    "                \n",
    "                by identifying 4-5 key sentences from the original text and \n",
    "                \n",
    "                DO NOT rewrite or paraphrase the content.\n",
    "                \n",
    "                Text is delimited by triple backticks. \n",
    "\n",
    "                TEXT: ```{record['article']}```\n",
    "            \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "        model=model, \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a LLM trained by OpenAI.\"},\n",
    "            {\"role\": \"user\", \"content\": ext_pmt}\n",
    "\n",
    "        ],\n",
    "        max_tokens = max_tokens,\n",
    "        temperature = temp,\n",
    "        n=1,\n",
    "        )\n",
    "        summary = response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        summary = \"ERROR\"\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to each row of the dataframe\n",
    "\n",
    "sample_test_df['gpt4_ets_low_temp_30_words'] = sample_test_df.apply(lambda rec: generate_GPT_Ext_Summary(rec, 'gpt4', 0, 30), axis=1)\n",
    "sample_test_df['gpt4_ets_med_temp_30_words'] = sample_test_df.apply(lambda rec: generate_GPT_Ext_Summary(rec, 'gpt4', 1, 30), axis=1)\n",
    "sample_test_df['gpt4-ets_high_temp_30_words'] = sample_test_df.apply(lambda rec: generate_GPT_Ext_Summary(rec, 'gpt4', 2, 30), axis=1)\n",
    "\n",
    "sample_test_df['gpt35_ets_low_temp_30_words'] = sample_test_df.apply(lambda rec: generate_GPT_Ext_Summary(rec, 'gpt35', 0, 30), axis=1)\n",
    "sample_test_df['gpt35_ets_med_temp_30_words'] = sample_test_df.apply(lambda rec: generate_GPT_Ext_Summary(rec, 'gpt35', 1, 30), axis=1)\n",
    "sample_test_df['gpt35-ets_high_temp_30_words'] = sample_test_df.apply(lambda rec: generate_GPT_Ext_Summary(rec, 'gpt35', 2, 30), axis=1)\n",
    "\n",
    "sample_test_df['gpt4_ets_low_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_Ext_Summary(rec, 'gpt4', 0, 60), axis=1)\n",
    "sample_test_df['gpt4_ets_med_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_Ext_Summary(rec, 'gpt4', 1, 60), axis=1)\n",
    "sample_test_df['gpt4-ets_high_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_Ext_Summary(rec, 'gpt4', 2, 60), axis=1)\n",
    "\n",
    "sample_test_df['gpt35_ets_low_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_Ext_Summary(rec, 'gpt35', 0, 60), axis=1)\n",
    "sample_test_df['gpt35_ets_med_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_Ext_Summary(rec, 'gpt35', 1, 60), axis=1)\n",
    "sample_test_df['gpt35-ets_high_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_Ext_Summary(rec, 'gpt35', 2, 60), axis=1)\n",
    "\n",
    "sample_test_df['gpt4_ets_low_temp_90_words'] = sample_test_df.apply(lambda rec: generate_GPT_Ext_Summary(rec, 'gpt4', 0, 90), axis=1)\n",
    "sample_test_df['gpt4_ets_med_temp_90_words'] = sample_test_df.apply(lambda rec: generate_GPT_Ext_Summary(rec, 'gpt4', 1, 90), axis=1)\n",
    "sample_test_df['gpt4-ets_high_temp_90_words'] = sample_test_df.apply(lambda rec: generate_GPT_Ext_Summary(rec, 'gpt4', 2, 90), axis=1)\n",
    "\n",
    "sample_test_df['gpt35_ets_low_temp_90_words'] = sample_test_df.apply(lambda rec: generate_GPT_Ext_Summary(rec, 'gpt35', 0, 90), axis=1)\n",
    "sample_test_df['gpt35_ets_med_temp_90_words'] = sample_test_df.apply(lambda rec: generate_GPT_Ext_Summary(rec, 'gpt35', 1, 90), axis=1)\n",
    "sample_test_df['gpt35-ets_high_temp_90_words'] = sample_test_df.apply(lambda rec: generate_GPT_Ext_Summary(rec, 'gpt35', 2, 90), axis=1)\n",
    "\n",
    "sample_test_df['gpt4_ets_low_temp_120_words'] = sample_test_df.apply(lambda rec: generate_GPT_Ext_Summary(rec, 'gpt4', 0, 120), axis=1)\n",
    "sample_test_df['gpt4_ets_med_temp_120_words'] = sample_test_df.apply(lambda rec: generate_GPT_Ext_Summary(rec, 'gpt4', 1, 120), axis=1)\n",
    "sample_test_df['gpt4-ets_high_temp_120_words'] = sample_test_df.apply(lambda rec: generate_GPT_Ext_Summary(rec, 'gpt4', 2, 120), axis=1)\n",
    "\n",
    "sample_test_df['gpt35_ets_low_temp_120_words'] = sample_test_df.apply(lambda rec: generate_GPT_Ext_Summary(rec, 'gpt35', 0, 120), axis=1)\n",
    "sample_test_df['gpt35_ets_med_temp_120_words'] = sample_test_df.apply(lambda rec: generate_GPT_Ext_Summary(rec, 'gpt35', 1, 120), axis=1)\n",
    "sample_test_df['gpt35-ets_high_temp_120_words'] = sample_test_df.apply(lambda rec: generate_GPT_Ext_Summary(rec, 'gpt35', 2, 120), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge_dict  {'rouge1': 0.3927068723702665, 'rouge2': 0.1348314606741573, 'rougeL': 0.2187938288920056, 'rougeLsum': 0.2187938288920056}\n",
      "rouge_dict  {'rouge1': 0.38450704225352106, 'rouge2': 0.1086036671368124, 'rougeL': 0.19999999999999998, 'rougeLsum': 0.19999999999999998}\n",
      "rouge_dict  {'rouge1': 0.35090252707581226, 'rouge2': 0.11713665943600866, 'rougeL': 0.1891696750902527, 'rougeLsum': 0.1891696750902527}\n",
      "rouge_dict  {'rouge1': 0.4206848357791754, 'rouge2': 0.1497550734779566, 'rougeL': 0.2180293501048218, 'rougeLsum': 0.2180293501048218}\n",
      "rouge_dict  {'rouge1': 0.38461538461538464, 'rouge2': 0.10504201680672269, 'rougeL': 0.20139860139860138, 'rougeLsum': 0.20139860139860138}\n",
      "rouge_dict  {'rouge1': 0.1457725947521866, 'rouge2': 0.02335766423357664, 'rougeL': 0.07434402332361517, 'rougeLsum': 0.07434402332361517}\n",
      "rouge_dict  {'rouge1': 0.4985706117781589, 'rouge2': 0.15798511734401832, 'rougeL': 0.23441966838193257, 'rougeLsum': 0.23441966838193257}\n",
      "rouge_dict  {'rouge1': 0.4919908466819222, 'rouge2': 0.16036655211912945, 'rougeL': 0.2391304347826087, 'rougeLsum': 0.2391304347826087}\n",
      "rouge_dict  {'rouge1': 0.4672176308539945, 'rouge2': 0.14671814671814673, 'rougeL': 0.21707988980716256, 'rougeLsum': 0.21707988980716256}\n",
      "rouge_dict  {'rouge1': 0.4917491749174917, 'rouge2': 0.15748898678414097, 'rougeL': 0.23762376237623764, 'rougeLsum': 0.23762376237623764}\n",
      "rouge_dict  {'rouge1': 0.4797297297297297, 'rouge2': 0.1319052987598647, 'rougeL': 0.21621621621621623, 'rougeLsum': 0.21621621621621623}\n",
      "rouge_dict  {'rouge1': 0.18711484593837535, 'rouge2': 0.024677509814918674, 'rougeL': 0.08403361344537814, 'rougeLsum': 0.08403361344537814}\n",
      "rouge_dict  {'rouge1': 0.4853828306264501, 'rouge2': 0.14955875522526707, 'rougeL': 0.21160092807424594, 'rougeLsum': 0.21160092807424594}\n",
      "rouge_dict  {'rouge1': 0.4988784208165097, 'rouge2': 0.1742254153569825, 'rougeL': 0.24136384028712426, 'rougeLsum': 0.24136384028712426}\n",
      "rouge_dict  {'rouge1': 0.3985637342908438, 'rouge2': 0.11769991015274035, 'rougeL': 0.18043087971274685, 'rougeLsum': 0.18043087971274685}\n",
      "rouge_dict  {'rouge1': 0.47501146263182026, 'rouge2': 0.15052776502983017, 'rougeL': 0.22925263640531868, 'rougeLsum': 0.22925263640531868}\n",
      "rouge_dict  {'rouge1': 0.4895042429656097, 'rouge2': 0.16897630755476084, 'rougeL': 0.2251004912907548, 'rougeLsum': 0.2251004912907548}\n",
      "rouge_dict  {'rouge1': 0.11874707807386628, 'rouge2': 0.011230697239120261, 'rougeL': 0.049555867227676485, 'rougeLsum': 0.049555867227676485}\n",
      "rouge_dict  {'rouge1': 0.46026011560693647, 'rouge2': 0.18582791033984092, 'rougeL': 0.2507225433526012, 'rougeLsum': 0.2507225433526012}\n",
      "rouge_dict  {'rouge1': 0.46053110221898874, 'rouge2': 0.17546414270112848, 'rougeL': 0.2371771553292106, 'rougeLsum': 0.2371771553292106}\n",
      "rouge_dict  {'rouge1': 0.4182450715114031, 'rouge2': 0.13539651837524178, 'rougeL': 0.18477000386548123, 'rougeLsum': 0.18477000386548123}\n",
      "rouge_dict  {'rouge1': 0.438821752265861, 'rouge2': 0.13907785336356765, 'rougeL': 0.2167673716012085, 'rougeLsum': 0.2167673716012085}\n",
      "rouge_dict  {'rouge1': 0.43840714551544474, 'rouge2': 0.14823091247672254, 'rougeL': 0.21883141049497581, 'rougeLsum': 0.21883141049497581}\n",
      "rouge_dict  {'rouge1': 0.14321398834304744, 'rouge2': 0.020833333333333332, 'rougeL': 0.058284762697751874, 'rougeLsum': 0.058284762697751874}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt4_ets_low_temp_30_words</th>\n",
       "      <td>0.392707</td>\n",
       "      <td>0.134831</td>\n",
       "      <td>0.218794</td>\n",
       "      <td>0.218794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4_ets_med_temp_30_words</th>\n",
       "      <td>0.384507</td>\n",
       "      <td>0.108604</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4-ets_high_temp_30_words</th>\n",
       "      <td>0.350903</td>\n",
       "      <td>0.117137</td>\n",
       "      <td>0.189170</td>\n",
       "      <td>0.189170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35_ets_low_temp_30_words</th>\n",
       "      <td>0.420685</td>\n",
       "      <td>0.149755</td>\n",
       "      <td>0.218029</td>\n",
       "      <td>0.218029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35_ets_med_temp_30_words</th>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.105042</td>\n",
       "      <td>0.201399</td>\n",
       "      <td>0.201399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-ets_high_temp_30_words</th>\n",
       "      <td>0.145773</td>\n",
       "      <td>0.023358</td>\n",
       "      <td>0.074344</td>\n",
       "      <td>0.074344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4_ets_low_temp_60_words</th>\n",
       "      <td>0.498571</td>\n",
       "      <td>0.157985</td>\n",
       "      <td>0.234420</td>\n",
       "      <td>0.234420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4_ets_med_temp_60_words</th>\n",
       "      <td>0.491991</td>\n",
       "      <td>0.160367</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.239130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4-ets_high_temp_60_words</th>\n",
       "      <td>0.467218</td>\n",
       "      <td>0.146718</td>\n",
       "      <td>0.217080</td>\n",
       "      <td>0.217080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35_ets_low_temp_60_words</th>\n",
       "      <td>0.491749</td>\n",
       "      <td>0.157489</td>\n",
       "      <td>0.237624</td>\n",
       "      <td>0.237624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35_ets_med_temp_60_words</th>\n",
       "      <td>0.479730</td>\n",
       "      <td>0.131905</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.216216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-ets_high_temp_60_words</th>\n",
       "      <td>0.187115</td>\n",
       "      <td>0.024678</td>\n",
       "      <td>0.084034</td>\n",
       "      <td>0.084034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4_ets_low_temp_90_words</th>\n",
       "      <td>0.485383</td>\n",
       "      <td>0.149559</td>\n",
       "      <td>0.211601</td>\n",
       "      <td>0.211601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4_ets_med_temp_90_words</th>\n",
       "      <td>0.498878</td>\n",
       "      <td>0.174225</td>\n",
       "      <td>0.241364</td>\n",
       "      <td>0.241364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4-ets_high_temp_90_words</th>\n",
       "      <td>0.398564</td>\n",
       "      <td>0.117700</td>\n",
       "      <td>0.180431</td>\n",
       "      <td>0.180431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35_ets_low_temp_90_words</th>\n",
       "      <td>0.475011</td>\n",
       "      <td>0.150528</td>\n",
       "      <td>0.229253</td>\n",
       "      <td>0.229253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35_ets_med_temp_90_words</th>\n",
       "      <td>0.489504</td>\n",
       "      <td>0.168976</td>\n",
       "      <td>0.225100</td>\n",
       "      <td>0.225100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-ets_high_temp_90_words</th>\n",
       "      <td>0.118747</td>\n",
       "      <td>0.011231</td>\n",
       "      <td>0.049556</td>\n",
       "      <td>0.049556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4_ets_low_temp_120_words</th>\n",
       "      <td>0.460260</td>\n",
       "      <td>0.185828</td>\n",
       "      <td>0.250723</td>\n",
       "      <td>0.250723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4_ets_med_temp_120_words</th>\n",
       "      <td>0.460531</td>\n",
       "      <td>0.175464</td>\n",
       "      <td>0.237177</td>\n",
       "      <td>0.237177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4-ets_high_temp_120_words</th>\n",
       "      <td>0.418245</td>\n",
       "      <td>0.135397</td>\n",
       "      <td>0.184770</td>\n",
       "      <td>0.184770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35_ets_low_temp_120_words</th>\n",
       "      <td>0.438822</td>\n",
       "      <td>0.139078</td>\n",
       "      <td>0.216767</td>\n",
       "      <td>0.216767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35_ets_med_temp_120_words</th>\n",
       "      <td>0.438407</td>\n",
       "      <td>0.148231</td>\n",
       "      <td>0.218831</td>\n",
       "      <td>0.218831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-ets_high_temp_120_words</th>\n",
       "      <td>0.143214</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.058285</td>\n",
       "      <td>0.058285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 rouge1    rouge2    rougeL  rougeLsum\n",
       "gpt4_ets_low_temp_30_words     0.392707  0.134831  0.218794   0.218794\n",
       "gpt4_ets_med_temp_30_words     0.384507  0.108604  0.200000   0.200000\n",
       "gpt4-ets_high_temp_30_words    0.350903  0.117137  0.189170   0.189170\n",
       "gpt35_ets_low_temp_30_words    0.420685  0.149755  0.218029   0.218029\n",
       "gpt35_ets_med_temp_30_words    0.384615  0.105042  0.201399   0.201399\n",
       "gpt35-ets_high_temp_30_words   0.145773  0.023358  0.074344   0.074344\n",
       "gpt4_ets_low_temp_60_words     0.498571  0.157985  0.234420   0.234420\n",
       "gpt4_ets_med_temp_60_words     0.491991  0.160367  0.239130   0.239130\n",
       "gpt4-ets_high_temp_60_words    0.467218  0.146718  0.217080   0.217080\n",
       "gpt35_ets_low_temp_60_words    0.491749  0.157489  0.237624   0.237624\n",
       "gpt35_ets_med_temp_60_words    0.479730  0.131905  0.216216   0.216216\n",
       "gpt35-ets_high_temp_60_words   0.187115  0.024678  0.084034   0.084034\n",
       "gpt4_ets_low_temp_90_words     0.485383  0.149559  0.211601   0.211601\n",
       "gpt4_ets_med_temp_90_words     0.498878  0.174225  0.241364   0.241364\n",
       "gpt4-ets_high_temp_90_words    0.398564  0.117700  0.180431   0.180431\n",
       "gpt35_ets_low_temp_90_words    0.475011  0.150528  0.229253   0.229253\n",
       "gpt35_ets_med_temp_90_words    0.489504  0.168976  0.225100   0.225100\n",
       "gpt35-ets_high_temp_90_words   0.118747  0.011231  0.049556   0.049556\n",
       "gpt4_ets_low_temp_120_words    0.460260  0.185828  0.250723   0.250723\n",
       "gpt4_ets_med_temp_120_words    0.460531  0.175464  0.237177   0.237177\n",
       "gpt4-ets_high_temp_120_words   0.418245  0.135397  0.184770   0.184770\n",
       "gpt35_ets_low_temp_120_words   0.438822  0.139078  0.216767   0.216767\n",
       "gpt35_ets_med_temp_120_words   0.438407  0.148231  0.218831   0.218831\n",
       "gpt35-ets_high_temp_120_words  0.143214  0.020833  0.058285   0.058285"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "models = [\n",
    "\n",
    "          'gpt4_ets_low_temp_30_words', \n",
    "          'gpt4_ets_med_temp_30_words' , \n",
    "          'gpt4-ets_high_temp_30_words',\n",
    "          'gpt35_ets_low_temp_30_words', \n",
    "          'gpt35_ets_med_temp_30_words', \n",
    "          'gpt35-ets_high_temp_30_words',\n",
    "    \n",
    "          'gpt4_ets_low_temp_60_words', \n",
    "          'gpt4_ets_med_temp_60_words' , \n",
    "          'gpt4-ets_high_temp_60_words',\n",
    "          'gpt35_ets_low_temp_60_words', \n",
    "          'gpt35_ets_med_temp_60_words', \n",
    "          'gpt35-ets_high_temp_60_words',\n",
    "\n",
    "          'gpt4_ets_low_temp_90_words', \n",
    "          'gpt4_ets_med_temp_90_words' , \n",
    "          'gpt4-ets_high_temp_90_words',\n",
    "          'gpt35_ets_low_temp_90_words', \n",
    "          'gpt35_ets_med_temp_90_words', \n",
    "          'gpt35-ets_high_temp_90_words',\n",
    "\n",
    "          'gpt4_ets_low_temp_120_words', \n",
    "          'gpt4_ets_med_temp_120_words' , \n",
    "          'gpt4-ets_high_temp_120_words',\n",
    "          'gpt35_ets_low_temp_120_words', \n",
    "          'gpt35_ets_med_temp_120_words', \n",
    "          'gpt35-ets_high_temp_120_words',\n",
    "         ]\n",
    "\n",
    "records = []\n",
    "\n",
    "for model_name in models:\n",
    "    rouge_metric.add(prediction = sample_test_df[model_name], reference = sample_test_df['highlights'])\n",
    "    score = rouge_metric.compute()\n",
    "    rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n",
    "    print('rouge_dict ', rouge_dict )\n",
    "    records.append(rouge_dict)\n",
    "\n",
    "metrics_df = pd.DataFrame.from_records(records, index = models )\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv('./output/GPT_ETS_PT_Metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT ETS Few Shots Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_df = pd.read_csv('./input/test.csv', nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computation of Length of article and hughlights\n",
    "def article_len(row):\n",
    "    return len(row['article'].split())\n",
    "\n",
    "def highlights_len(row):\n",
    "    return len(row['highlights'].split())\n",
    "\n",
    "\n",
    "sample_test_df['article_len'] = sample_test_df.apply(lambda r: article_len(r), axis= 1)\n",
    "sample_test_df['highlights_len'] = sample_test_df.apply(lambda r: highlights_len(r), axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_article_size_df = sample_test_df.sort_values('article_len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_df = sample_test_df[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_GPT_ETS_1_Shot(record, model, temp, max_tokens):\n",
    "    summary = \"\"\n",
    "    ext_pmt = f\"\"\"\n",
    "                Your task is to Generate extractive summary from the original text, \n",
    "                \n",
    "                by identifying 4-5 key sentences from the original text and \n",
    "                \n",
    "                DO NOT rewrite or paraphrase the content.\n",
    "                \n",
    "                Text is delimited by triple backticks. \n",
    "\n",
    "                TEXT: ```{record['article']}```\n",
    "            \"\"\"\n",
    "    \n",
    "    #create a concise summary by selecting and combining key sentences from the original text : {text}\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "        model=model, # model = \"deployment_name\".\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a LLM trained by OpenAI.\"},\n",
    "            {\"role\":\"user\",\"content\":sorted_by_article_size_df.iloc[0]['article']},\n",
    "            {\"role\":\"assistant\",\"content\":sorted_by_article_size_df.iloc[0]['highlights']},\n",
    "            {\"role\": \"user\", \"content\": ext_pmt},\n",
    "            ],\n",
    "        max_tokens = max_tokens,\n",
    "        temperature = temp, #top_p=.9\n",
    "        n=1,\n",
    "        )\n",
    "        summary = response.choices[0].message.content\n",
    "    except Exception:\n",
    "        print(Exception)\n",
    "        summary = \"ERROR\"\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def generate_GPT_ETS_3_Shots(record, model, temp, max_tokens):\n",
    "    summary = \"\"\n",
    "    \n",
    "    ext_pmt = f\"\"\"\n",
    "                Your task is to Generate extractive summary from the original text, \n",
    "                \n",
    "                by identifying 4-5 key sentences from the original text and \n",
    "                \n",
    "                DO NOT rewrite or paraphrase the content.\n",
    "                \n",
    "                Text is delimited by triple backticks. \n",
    "\n",
    "                TEXT: ```{record['article']}```\n",
    "            \"\"\"\n",
    "    \n",
    "    #create a concise summary by selecting and combining key sentences from the original text : {text}\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "        model=model, # model = \"deployment_name\".\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a LLM trained by OpenAI.\"},\n",
    "\n",
    "            {\"role\":\"user\",\"content\":sorted_by_article_size_df.iloc[0]['article']},\n",
    "            {\"role\":\"assistant\",\"content\":sorted_by_article_size_df.iloc[0]['highlights']},\n",
    "            \n",
    "            {\"role\":\"user\",\"content\":sorted_by_article_size_df.iloc[1]['article']},\n",
    "            {\"role\":\"assistant\",\"content\":sorted_by_article_size_df.iloc[1]['highlights']},\n",
    "\n",
    "            {\"role\":\"user\",\"content\":sorted_by_article_size_df.iloc[2]['article']},\n",
    "            {\"role\":\"assistant\",\"content\":sorted_by_article_size_df.iloc[2]['highlights']},\n",
    "            \n",
    "            {\"role\": \"user\", \"content\": ext_pmt},\n",
    "            \n",
    "        ],\n",
    "        max_tokens = max_tokens,\n",
    "        temperature = temp, #top_p=.9\n",
    "        n=1,\n",
    "        )\n",
    "        summary = response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        summary = \"ERROR\"\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def generate_GPT_ETS_5_Shots(record, model, temp, max_tokens):\n",
    "    summary = \"\"\n",
    "       \n",
    "    ext_pmt = f\"\"\"\n",
    "                Your task is to Generate extractive summary from the original text, \n",
    "                \n",
    "                by identifying 4-5 key sentences from the original text and \n",
    "                \n",
    "                DO NOT rewrite or paraphrase the content.\n",
    "                \n",
    "                Text is delimited by triple backticks. \n",
    "\n",
    "                TEXT: ```{record['article']}```\n",
    "            \"\"\"\n",
    "    #print('ARTICLE ', record['article'][:200])\n",
    "        #create a concise summary by selecting and combining key sentences from the original text : {text}\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model= model, # model = \"deployment_name\".\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a LLM trained by OpenAI.\"},\n",
    "\n",
    "                {\"role\":\"user\",\"content\":sorted_by_article_size_df.iloc[0]['article']},\n",
    "                {\"role\":\"assistant\",\"content\":sorted_by_article_size_df.iloc[0]['highlights']},\n",
    "\n",
    "                {\"role\":\"user\",\"content\":sorted_by_article_size_df.iloc[1]['article']},\n",
    "                {\"role\":\"assistant\",\"content\":sorted_by_article_size_df.iloc[1]['highlights']},\n",
    "\n",
    "                {\"role\":\"user\",\"content\":sorted_by_article_size_df.iloc[2]['article']},\n",
    "                {\"role\":\"assistant\",\"content\":sorted_by_article_size_df.iloc[2]['highlights']},\n",
    "\n",
    "                {\"role\":\"user\",\"content\":sorted_by_article_size_df.iloc[3]['article']},\n",
    "                {\"role\":\"assistant\",\"content\":sorted_by_article_size_df.iloc[3]['highlights']},\n",
    "\n",
    "                {\"role\":\"user\",\"content\":sorted_by_article_size_df.iloc[4]['article']},\n",
    "                {\"role\":\"assistant\",\"content\":sorted_by_article_size_df.iloc[4]['highlights']},\n",
    "\n",
    "                {\"role\": \"user\", \"content\": ext_pmt},\n",
    "\n",
    "            ],\n",
    "            max_tokens = max_tokens,\n",
    "            temperature = temp, \n",
    "            top_p= 0.9,\n",
    "            presence_penalty=0.01,\n",
    "            n=1,\n",
    "            )\n",
    "        #print(\"Summary - \", response.choices[0].message.content)\n",
    "        summary = response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        summary = \"ERROR\"\n",
    "  \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n",
      "<class 'Exception'>\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to each row of the dataframe\n",
    "sample_test_df['gpt4-ets-5_shots_low_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_ETS_5_Shots(rec, 'gpt4', 0, 60), axis=1)\n",
    "sample_test_df['gpt4-ets-5_shots_med_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_ETS_5_Shots(rec, 'gpt4', 1, 60), axis=1)\n",
    "sample_test_df['gpt4-ets-5_shots_high_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_ETS_5_Shots(rec, 'gpt4', 2, 60), axis=1)\n",
    "\n",
    "sample_test_df['gpt35-ets-5_shots_low_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_ETS_5_Shots(rec, 'gpt35', 0, 60), axis=1)\n",
    "sample_test_df['gpt35-ets-5_shots_med_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_ETS_5_Shots(rec, 'gpt35', 1, 60), axis=1)\n",
    "sample_test_df['gpt35-ets-5_shots_high_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_ETS_5_Shots(rec, 'gpt35', 2, 60), axis=1)\n",
    "\n",
    "sample_test_df['gpt4-ets-3_shots_low_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_ETS_3_Shots(rec, 'gpt4', 0, 60), axis=1)\n",
    "sample_test_df['gpt4-ets-3_shots_med_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_ETS_3_Shots(rec, 'gpt4', 1, 60), axis=1)\n",
    "sample_test_df['gpt4-ets-3_shots_high_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_ETS_3_Shots(rec, 'gpt4', 2, 60), axis=1)\n",
    "\n",
    "sample_test_df['gpt35-ets-3_shots_low_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_ETS_3_Shots(rec, 'gpt35', 0, 60), axis=1)\n",
    "sample_test_df['gpt35-ets-3_shots_med_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_ETS_3_Shots(rec, 'gpt35', 1, 60), axis=1)\n",
    "sample_test_df['gpt35-ets-3_shots_high_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_ETS_3_Shots(rec, 'gpt35', 2, 60), axis=1)\n",
    "\n",
    "sample_test_df['gpt4-ets-1_shot_low_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_ETS_1_Shot(rec, 'gpt4', 0, 60), axis=1)\n",
    "sample_test_df['gpt4-ets-1_shot_med_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_ETS_1_Shot(rec, 'gpt4', 1, 60), axis=1)\n",
    "sample_test_df['gpt4-ets-1_shot_high_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_ETS_1_Shot(rec, 'gpt4', 2, 60), axis=1)\n",
    "\n",
    "sample_test_df['gpt35-ets-1_shot_low_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_ETS_1_Shot(rec, 'gpt35', 0, 60), axis=1)\n",
    "sample_test_df['gpt35-ets-1_shot_med_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_ETS_1_Shot(rec, 'gpt35', 1, 60), axis=1)\n",
    "sample_test_df['gpt35-ets-1_shot_high_temp_60_words'] = sample_test_df.apply(lambda rec: generate_GPT_ETS_1_Shot(rec, 'gpt35', 2, 60), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge_dict  {'rouge1': 0.4980147475893364, 'rouge2': 0.17603634298693924, 'rougeL': 0.24503686897334087, 'rougeLsum': 0.24503686897334087}\n",
      "rouge_dict  {'rouge1': 0.4766355140186916, 'rouge2': 0.15789473684210523, 'rougeL': 0.2336448598130841, 'rougeLsum': 0.2336448598130841}\n",
      "rouge_dict  {'rouge1': 0.49144811858608894, 'rouge2': 0.17579908675799086, 'rougeL': 0.2371721778791334, 'rougeLsum': 0.2371721778791334}\n",
      "rouge_dict  {'rouge1': 0.5016987542468856, 'rouge2': 0.1598639455782313, 'rougeL': 0.23895809739524349, 'rougeLsum': 0.23895809739524349}\n",
      "rouge_dict  {'rouge1': 0.4923413566739606, 'rouge2': 0.1325301204819277, 'rougeL': 0.22975929978118162, 'rougeLsum': 0.22975929978118162}\n",
      "rouge_dict  {'rouge1': 0.47287264420331243, 'rouge2': 0.12464265294453973, 'rougeL': 0.20902341519131926, 'rougeLsum': 0.20902341519131926}\n",
      "rouge_dict  {'rouge1': 0.49885583524027455, 'rouge2': 0.15693012600229095, 'rougeL': 0.2322654462242563, 'rougeLsum': 0.2322654462242563}\n",
      "rouge_dict  {'rouge1': 0.49230769230769234, 'rouge2': 0.15876777251184834, 'rougeL': 0.23550295857988165, 'rougeLsum': 0.23550295857988165}\n",
      "rouge_dict  {'rouge1': 0.4409356725146199, 'rouge2': 0.16393442622950816, 'rougeL': 0.2198830409356725, 'rougeLsum': 0.2198830409356725}\n",
      "rouge_dict  {'rouge1': 0.5270718232044198, 'rouge2': 0.18141592920353983, 'rougeL': 0.25193370165745854, 'rougeLsum': 0.25193370165745854}\n",
      "rouge_dict  {'rouge1': 0.4784580498866213, 'rouge2': 0.13280363223609537, 'rougeL': 0.21428571428571427, 'rougeLsum': 0.21428571428571427}\n",
      "rouge_dict  {'rouge1': 0.09701928696668614, 'rouge2': 0.007021650087770626, 'rougeL': 0.05143191116306253, 'rougeLsum': 0.05143191116306253}\n",
      "rouge_dict  {'rouge1': 0.49326303456356185, 'rouge2': 0.16656891495601175, 'rougeL': 0.23901581722319862, 'rougeLsum': 0.23901581722319862}\n",
      "rouge_dict  {'rouge1': 0.5037463976945246, 'rouge2': 0.1569532602423543, 'rougeL': 0.22478386167146974, 'rougeLsum': 0.22478386167146974}\n",
      "rouge_dict  {'rouge1': 0.476027397260274, 'rouge2': 0.14857142857142858, 'rougeL': 0.21575342465753428, 'rougeLsum': 0.21575342465753428}\n",
      "rouge_dict  {'rouge1': 0.5352422907488986, 'rouge2': 0.18302094818081588, 'rougeL': 0.2665198237885463, 'rougeLsum': 0.2665198237885463}\n",
      "rouge_dict  {'rouge1': 0.5195689166193987, 'rouge2': 0.17262918796138557, 'rougeL': 0.25524673851389673, 'rougeLsum': 0.25524673851389673}\n",
      "rouge_dict  {'rouge1': 0.12914285714285711, 'rouge2': 0.014874141876430207, 'rougeL': 0.054857142857142854, 'rougeLsum': 0.054857142857142854}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt4-ets-5_shots_low_temp_60_words</th>\n",
       "      <td>0.498015</td>\n",
       "      <td>0.176036</td>\n",
       "      <td>0.245037</td>\n",
       "      <td>0.245037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4-ets-5_shots_med_temp_60_words</th>\n",
       "      <td>0.476636</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.233645</td>\n",
       "      <td>0.233645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4-ets-5_shots_high_temp_60_words</th>\n",
       "      <td>0.491448</td>\n",
       "      <td>0.175799</td>\n",
       "      <td>0.237172</td>\n",
       "      <td>0.237172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-ets-5_shots_low_temp_60_words</th>\n",
       "      <td>0.501699</td>\n",
       "      <td>0.159864</td>\n",
       "      <td>0.238958</td>\n",
       "      <td>0.238958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-ets-5_shots_med_temp_60_words</th>\n",
       "      <td>0.492341</td>\n",
       "      <td>0.132530</td>\n",
       "      <td>0.229759</td>\n",
       "      <td>0.229759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-ets-5_shots_high_temp_60_words</th>\n",
       "      <td>0.472873</td>\n",
       "      <td>0.124643</td>\n",
       "      <td>0.209023</td>\n",
       "      <td>0.209023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4-ets-3_shots_low_temp_60_words</th>\n",
       "      <td>0.498856</td>\n",
       "      <td>0.156930</td>\n",
       "      <td>0.232265</td>\n",
       "      <td>0.232265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4-ets-3_shots_med_temp_60_words</th>\n",
       "      <td>0.492308</td>\n",
       "      <td>0.158768</td>\n",
       "      <td>0.235503</td>\n",
       "      <td>0.235503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4-ets-3_shots_high_temp_60_words</th>\n",
       "      <td>0.440936</td>\n",
       "      <td>0.163934</td>\n",
       "      <td>0.219883</td>\n",
       "      <td>0.219883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-ets-3_shots_low_temp_60_words</th>\n",
       "      <td>0.527072</td>\n",
       "      <td>0.181416</td>\n",
       "      <td>0.251934</td>\n",
       "      <td>0.251934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-ets-3_shots_med_temp_60_words</th>\n",
       "      <td>0.478458</td>\n",
       "      <td>0.132804</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-ets-3_shots_high_temp_60_words</th>\n",
       "      <td>0.097019</td>\n",
       "      <td>0.007022</td>\n",
       "      <td>0.051432</td>\n",
       "      <td>0.051432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4-ets-1_shot_low_temp_60_words</th>\n",
       "      <td>0.493263</td>\n",
       "      <td>0.166569</td>\n",
       "      <td>0.239016</td>\n",
       "      <td>0.239016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4-ets-1_shot_med_temp_60_words</th>\n",
       "      <td>0.503746</td>\n",
       "      <td>0.156953</td>\n",
       "      <td>0.224784</td>\n",
       "      <td>0.224784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4-ets-1_shot_high_temp_60_words</th>\n",
       "      <td>0.476027</td>\n",
       "      <td>0.148571</td>\n",
       "      <td>0.215753</td>\n",
       "      <td>0.215753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-ets-1_shot_low_temp_60_words</th>\n",
       "      <td>0.535242</td>\n",
       "      <td>0.183021</td>\n",
       "      <td>0.266520</td>\n",
       "      <td>0.266520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-ets-1_shot_med_temp_60_words</th>\n",
       "      <td>0.519569</td>\n",
       "      <td>0.172629</td>\n",
       "      <td>0.255247</td>\n",
       "      <td>0.255247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-ets-1_shot_high_temp_60_words</th>\n",
       "      <td>0.129143</td>\n",
       "      <td>0.014874</td>\n",
       "      <td>0.054857</td>\n",
       "      <td>0.054857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        rouge1    rouge2    rougeL  rougeLsum\n",
       "gpt4-ets-5_shots_low_temp_60_words    0.498015  0.176036  0.245037   0.245037\n",
       "gpt4-ets-5_shots_med_temp_60_words    0.476636  0.157895  0.233645   0.233645\n",
       "gpt4-ets-5_shots_high_temp_60_words   0.491448  0.175799  0.237172   0.237172\n",
       "gpt35-ets-5_shots_low_temp_60_words   0.501699  0.159864  0.238958   0.238958\n",
       "gpt35-ets-5_shots_med_temp_60_words   0.492341  0.132530  0.229759   0.229759\n",
       "gpt35-ets-5_shots_high_temp_60_words  0.472873  0.124643  0.209023   0.209023\n",
       "gpt4-ets-3_shots_low_temp_60_words    0.498856  0.156930  0.232265   0.232265\n",
       "gpt4-ets-3_shots_med_temp_60_words    0.492308  0.158768  0.235503   0.235503\n",
       "gpt4-ets-3_shots_high_temp_60_words   0.440936  0.163934  0.219883   0.219883\n",
       "gpt35-ets-3_shots_low_temp_60_words   0.527072  0.181416  0.251934   0.251934\n",
       "gpt35-ets-3_shots_med_temp_60_words   0.478458  0.132804  0.214286   0.214286\n",
       "gpt35-ets-3_shots_high_temp_60_words  0.097019  0.007022  0.051432   0.051432\n",
       "gpt4-ets-1_shot_low_temp_60_words     0.493263  0.166569  0.239016   0.239016\n",
       "gpt4-ets-1_shot_med_temp_60_words     0.503746  0.156953  0.224784   0.224784\n",
       "gpt4-ets-1_shot_high_temp_60_words    0.476027  0.148571  0.215753   0.215753\n",
       "gpt35-ets-1_shot_low_temp_60_words    0.535242  0.183021  0.266520   0.266520\n",
       "gpt35-ets-1_shot_med_temp_60_words    0.519569  0.172629  0.255247   0.255247\n",
       "gpt35-ets-1_shot_high_temp_60_words   0.129143  0.014874  0.054857   0.054857"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "models = [\n",
    "\n",
    "          'gpt4-ets-5_shots_low_temp_60_words', \n",
    "          'gpt4-ets-5_shots_med_temp_60_words' , \n",
    "          'gpt4-ets-5_shots_high_temp_60_words',\n",
    "          'gpt35-ets-5_shots_low_temp_60_words', \n",
    "          'gpt35-ets-5_shots_med_temp_60_words' , \n",
    "          'gpt35-ets-5_shots_high_temp_60_words',\n",
    "\n",
    "          'gpt4-ets-3_shots_low_temp_60_words', \n",
    "          'gpt4-ets-3_shots_med_temp_60_words' , \n",
    "          'gpt4-ets-3_shots_high_temp_60_words',\n",
    "          'gpt35-ets-3_shots_low_temp_60_words', \n",
    "          'gpt35-ets-3_shots_med_temp_60_words' , \n",
    "          'gpt35-ets-3_shots_high_temp_60_words',\n",
    "    \n",
    "          'gpt4-ets-1_shot_low_temp_60_words', \n",
    "          'gpt4-ets-1_shot_med_temp_60_words' , \n",
    "          'gpt4-ets-1_shot_high_temp_60_words',\n",
    "          'gpt35-ets-1_shot_low_temp_60_words', \n",
    "          'gpt35-ets-1_shot_med_temp_60_words' , \n",
    "          'gpt35-ets-1_shot_high_temp_60_words',\n",
    "         ]\n",
    "\n",
    "records = []\n",
    "\n",
    "for model_name in models:\n",
    "    rouge_metric.add(prediction = sample_test_df[model_name], reference = sample_test_df['highlights'])\n",
    "    score = rouge_metric.compute()\n",
    "    rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n",
    "    print('rouge_dict ', rouge_dict )\n",
    "    records.append(rouge_dict)\n",
    "\n",
    "metrics_df = pd.DataFrame.from_records(records, index = models )\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt4-ets-5_shots_low_temp_60_words</th>\n",
       "      <td>0.498015</td>\n",
       "      <td>0.176036</td>\n",
       "      <td>0.245037</td>\n",
       "      <td>0.245037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4-ets-5_shots_med_temp_60_words</th>\n",
       "      <td>0.476636</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.233645</td>\n",
       "      <td>0.233645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4-ets-5_shots_high_temp_60_words</th>\n",
       "      <td>0.491448</td>\n",
       "      <td>0.175799</td>\n",
       "      <td>0.237172</td>\n",
       "      <td>0.237172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-ets-5_shots_low_temp_60_words</th>\n",
       "      <td>0.501699</td>\n",
       "      <td>0.159864</td>\n",
       "      <td>0.238958</td>\n",
       "      <td>0.238958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-ets-5_shots_med_temp_60_words</th>\n",
       "      <td>0.492341</td>\n",
       "      <td>0.132530</td>\n",
       "      <td>0.229759</td>\n",
       "      <td>0.229759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-ets-5_shots_high_temp_60_words</th>\n",
       "      <td>0.472873</td>\n",
       "      <td>0.124643</td>\n",
       "      <td>0.209023</td>\n",
       "      <td>0.209023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4-ets-3_shots_low_temp_60_words</th>\n",
       "      <td>0.498856</td>\n",
       "      <td>0.156930</td>\n",
       "      <td>0.232265</td>\n",
       "      <td>0.232265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4-ets-3_shots_med_temp_60_words</th>\n",
       "      <td>0.492308</td>\n",
       "      <td>0.158768</td>\n",
       "      <td>0.235503</td>\n",
       "      <td>0.235503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4-ets-3_shots_high_temp_60_words</th>\n",
       "      <td>0.440936</td>\n",
       "      <td>0.163934</td>\n",
       "      <td>0.219883</td>\n",
       "      <td>0.219883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-ets-3_shots_low_temp_60_words</th>\n",
       "      <td>0.527072</td>\n",
       "      <td>0.181416</td>\n",
       "      <td>0.251934</td>\n",
       "      <td>0.251934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-ets-3_shots_med_temp_60_words</th>\n",
       "      <td>0.478458</td>\n",
       "      <td>0.132804</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-ets-3_shots_high_temp_60_words</th>\n",
       "      <td>0.097019</td>\n",
       "      <td>0.007022</td>\n",
       "      <td>0.051432</td>\n",
       "      <td>0.051432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4-ets-1_shot_low_temp_60_words</th>\n",
       "      <td>0.493263</td>\n",
       "      <td>0.166569</td>\n",
       "      <td>0.239016</td>\n",
       "      <td>0.239016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4-ets-1_shot_med_temp_60_words</th>\n",
       "      <td>0.503746</td>\n",
       "      <td>0.156953</td>\n",
       "      <td>0.224784</td>\n",
       "      <td>0.224784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4-ets-1_shot_high_temp_60_words</th>\n",
       "      <td>0.476027</td>\n",
       "      <td>0.148571</td>\n",
       "      <td>0.215753</td>\n",
       "      <td>0.215753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-ets-1_shot_low_temp_60_words</th>\n",
       "      <td>0.535242</td>\n",
       "      <td>0.183021</td>\n",
       "      <td>0.266520</td>\n",
       "      <td>0.266520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-ets-1_shot_med_temp_60_words</th>\n",
       "      <td>0.519569</td>\n",
       "      <td>0.172629</td>\n",
       "      <td>0.255247</td>\n",
       "      <td>0.255247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt35-ets-1_shot_high_temp_60_words</th>\n",
       "      <td>0.129143</td>\n",
       "      <td>0.014874</td>\n",
       "      <td>0.054857</td>\n",
       "      <td>0.054857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        rouge1    rouge2    rougeL  rougeLsum\n",
       "gpt4-ets-5_shots_low_temp_60_words    0.498015  0.176036  0.245037   0.245037\n",
       "gpt4-ets-5_shots_med_temp_60_words    0.476636  0.157895  0.233645   0.233645\n",
       "gpt4-ets-5_shots_high_temp_60_words   0.491448  0.175799  0.237172   0.237172\n",
       "gpt35-ets-5_shots_low_temp_60_words   0.501699  0.159864  0.238958   0.238958\n",
       "gpt35-ets-5_shots_med_temp_60_words   0.492341  0.132530  0.229759   0.229759\n",
       "gpt35-ets-5_shots_high_temp_60_words  0.472873  0.124643  0.209023   0.209023\n",
       "gpt4-ets-3_shots_low_temp_60_words    0.498856  0.156930  0.232265   0.232265\n",
       "gpt4-ets-3_shots_med_temp_60_words    0.492308  0.158768  0.235503   0.235503\n",
       "gpt4-ets-3_shots_high_temp_60_words   0.440936  0.163934  0.219883   0.219883\n",
       "gpt35-ets-3_shots_low_temp_60_words   0.527072  0.181416  0.251934   0.251934\n",
       "gpt35-ets-3_shots_med_temp_60_words   0.478458  0.132804  0.214286   0.214286\n",
       "gpt35-ets-3_shots_high_temp_60_words  0.097019  0.007022  0.051432   0.051432\n",
       "gpt4-ets-1_shot_low_temp_60_words     0.493263  0.166569  0.239016   0.239016\n",
       "gpt4-ets-1_shot_med_temp_60_words     0.503746  0.156953  0.224784   0.224784\n",
       "gpt4-ets-1_shot_high_temp_60_words    0.476027  0.148571  0.215753   0.215753\n",
       "gpt35-ets-1_shot_low_temp_60_words    0.535242  0.183021  0.266520   0.266520\n",
       "gpt35-ets-1_shot_med_temp_60_words    0.519569  0.172629  0.255247   0.255247\n",
       "gpt35-ets-1_shot_high_temp_60_words   0.129143  0.014874  0.054857   0.054857"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv(\"./output/GPT_ETS_FS_Metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
